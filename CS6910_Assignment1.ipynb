{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " CS6910_Assignment1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d031c9716cbf42129fc4d99a19920144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a3b5120e4e647079cc3dc60f412fb83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3280bdd4d6ca40099e517d9106dd920b",
              "IPY_MODEL_121d3f9fae3b4a8eb14d29e9f0dcdc4c"
            ]
          }
        },
        "9a3b5120e4e647079cc3dc60f412fb83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3280bdd4d6ca40099e517d9106dd920b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_7590c045090843f0bdd3d04d68b02b4d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d2810d61fba4b8e84061dcd7b25dac5"
          }
        },
        "121d3f9fae3b4a8eb14d29e9f0dcdc4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14f3ab27220d40079521704ae4fde2a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66a0be5e4e554100b5b0e7b1d8fd78e0"
          }
        },
        "7590c045090843f0bdd3d04d68b02b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d2810d61fba4b8e84061dcd7b25dac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14f3ab27220d40079521704ae4fde2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66a0be5e4e554100b5b0e7b1d8fd78e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safikhanSoofiyani/CS6910-Assignment1/blob/main/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Feed Forward Neural Network</h1>\n",
        "\n",
        "Work Done by:<br>\n",
        "<ul> \n",
        "<li>Mohammed Safi Ur Rahman Khan - CS21M035 </li>\n",
        "<li>Vamsi Sai Krishna Malineni  - OE20S302 </li>"
      ],
      "metadata": {
        "id": "R5q7bg9s5L8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Importing necessary libraries.</h3>"
      ],
      "metadata": {
        "id": "s_fr1Bcc6eOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahmgbKIg2f0o"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy \n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import seaborn as sn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various Links used till now:\n",
        "<ul>\n",
        "<li> L2 Regularization - https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd\n",
        "</ul>"
      ],
      "metadata": {
        "id": "M3hgrdYAmz7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing wandb"
      ],
      "metadata": {
        "id": "OG-qZnHL7Gyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "id": "VVG9Nfzc7DLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3982ba-1ce5-4082-82ae-228e7215080d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 10.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 72.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144 kB 11.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparing dataset</h3>"
      ],
      "metadata": {
        "id": "GU1iINihIbYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data():\n",
        "\n",
        "  '''This function is used to load the data, define the class labels, performing\n",
        "      the train-test-validation split, normalizing the data, flattening each data\n",
        "      point, converting the class labels to one hot encoded vector.\n",
        "\n",
        "      It return all the split data sets '''\n",
        "\n",
        "\n",
        "  # Loading data from online source\n",
        "  (train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()\n",
        "\n",
        "  # Defining labels for data\n",
        "  num_classes = 10\n",
        "  labels=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "  print(\"Number of data points in train data (initially) - \", len(train_x))\n",
        "  print(\"Number of data points in test data (initially) - \", len(test_x))\n",
        "\n",
        "\n",
        "  #performing the train-validation split\n",
        "  train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=40)\n",
        "  \n",
        "\n",
        "  print(\"Shape of each image - 28x28\" )\n",
        "  image_shape=train_x.shape[1]*train_x.shape[2]\n",
        "  print(\"shape of each image (1D) - \",image_shape)\n",
        "  \n",
        "  train_image_count=len(train_x)\n",
        "  val_image_count = len(val_x)\n",
        "  test_image_count=len(test_x)\n",
        "  \n",
        "  # Creating a matrix of image data \n",
        "  # each image is represented as a row by flattening the matrix: converting (60000,28,28) tensor to (60000,784) matrix\n",
        "  X_train=np.zeros((train_image_count,image_shape))\n",
        "  X_val=np.zeros((val_image_count,image_shape))\n",
        "  X_test=np.zeros((test_image_count,image_shape))\n",
        "  \n",
        "  # converting the images into grayscale by normalizing\n",
        "  for i in range(train_image_count):\n",
        "    X_train[i]=(copy.deepcopy(train_x[i].flatten()))/255.0 \n",
        "  for i in range(val_image_count):\n",
        "    X_val[i]=(copy.deepcopy(val_x[i].flatten()))/255.0\n",
        "  for i in range(test_image_count):\n",
        "    X_test[i]=(copy.deepcopy(test_x[i].flatten()))/255.0\n",
        "  \n",
        "\n",
        "\n",
        "  #One hot encoding the label vectors to represent a probability distribution\n",
        "  y_train = np.zeros((train_y.size, 10))\n",
        "  y_train[np.arange(train_y.size), train_y] = 1\n",
        "\n",
        "  y_val = np.zeros((val_y.size, 10))\n",
        "  y_val[np.arange(val_y.size), val_y] = 1\n",
        "\n",
        "  y_test = np.zeros((test_y.size, 10))\n",
        "  y_test[np.arange(test_y.size), test_y] = 1\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  return X_train,X_val,X_test,y_train,y_val,y_test,labels\n",
        "  "
      ],
      "metadata": {
        "id": "uIanRhNjIZiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting images locally"
      ],
      "metadata": {
        "id": "kM6jDLswWRYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_locally():\n",
        "  xtrain,xval,xtest,ytrain,yval,ytest,labels=prepare_data()\n",
        "  # Creating training dataset\n",
        "  train=np.asarray(list(zip(xtrain,ytrain)))\n",
        "  # plotting a single image from each class\n",
        "  sample_images=[]\n",
        "  wandb_arr=[]\n",
        "  i=1\n",
        "  plt.suptitle(\"Plotting image of each class from Fashion MNIST Dataset\")\n",
        "\n",
        "  while(len(sample_images)!=10):\n",
        "    n=random.randrange(0,len(train))\n",
        "    lab_index=np.asarray(np.nonzero(train[n][1]))[0][0]\n",
        "    \n",
        "    if(lab_index not in sample_images):\n",
        "      plt.subplot(3,5,i)\n",
        "      sample_images.append(lab_index)\n",
        "      plt.title(labels[lab_index])\n",
        "      plt.axis(False)\n",
        "      plt.imshow(train[n][0].reshape((28,28)))\n",
        "      i=i+1"
      ],
      "metadata": {
        "id": "VmT6fEAtWQzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_locally()"
      ],
      "metadata": {
        "id": "wTFiRD0Zcu2Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "129a1df8-4b68-47b2-85fc-41a7ec3d7dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\safik\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting images via wandb"
      ],
      "metadata": {
        "id": "nRU6TlqpVSFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#********** Plotting images via wandb************\n",
        "\n",
        "# Enter the entity and project details from wandb.ai\n",
        "wandb.init(entity=\"safi-vamsi-cs6910\",project=\"Assignment 1\", name=\"log_images\")\n",
        "\n",
        "# Loading dataset\n",
        "xtrain,xval,xtest,ytrain,yval,ytest,labels=prepare_data()\n",
        "\n",
        "# Creating training dataset\n",
        "train=np.asarray(list(zip(xtrain,ytrain)))\n",
        "\n",
        "\n",
        "sample_images=[]\n",
        "wandb_arr=[]\n",
        "i=1\n",
        "plt.suptitle(\"Plotting image of each class from Fashion MNIST Dataset\")\n",
        "while(len(sample_images)!=10):\n",
        "  n=random.randrange(0,len(train))\n",
        "  lab_index=np.asarray(np.nonzero(train[n][1]))[0][0]\n",
        "  if(lab_index not in sample_images):\n",
        "    sample_images.append(lab_index)\n",
        "    wandb_arr.append(wandb.Image(train[n][0].reshape((28,28)),caption=labels[lab_index]))\n",
        "    i=i+1\n",
        "wandb.log({\"images\":wandb_arr})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "-9BOTbyz3rpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543,
          "referenced_widgets": [
            "d031c9716cbf42129fc4d99a19920144",
            "9a3b5120e4e647079cc3dc60f412fb83",
            "3280bdd4d6ca40099e517d9106dd920b",
            "121d3f9fae3b4a8eb14d29e9f0dcdc4c",
            "7590c045090843f0bdd3d04d68b02b4d",
            "3d2810d61fba4b8e84061dcd7b25dac5",
            "14f3ab27220d40079521704ae4fde2a1",
            "66a0be5e4e554100b5b0e7b1d8fd78e0"
          ]
        },
        "outputId": "d1ac674f-7539-428b-b3c9-c4068ca208f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/safikhan/CS6910%20Assignment%201/runs/3aq1e1pr\" target=\"_blank\">log_images</a></strong> to <a href=\"https://wandb.ai/safikhan/CS6910%20Assignment%201\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 187... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d031c9716cbf42129fc4d99a19920144",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">log_images</strong>: <a href=\"https://wandb.ai/safikhan/CS6910%20Assignment%201/runs/3aq1e1pr\" target=\"_blank\">https://wandb.ai/safikhan/CS6910%20Assignment%201/runs/3aq1e1pr</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220221_152653-3aq1e1pr/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()"
      ],
      "metadata": {
        "id": "ntsvInuZrJhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f47c2e2-d6b6-4690-e6ab-464382916a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Defining Various Utility functions</h3>"
      ],
      "metadata": {
        "id": "7OI2Z4lEK1Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Initialization Functions</h4>"
      ],
      "metadata": {
        "id": "cBekYpc66E6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Xavier(layer_sizes):\n",
        "  params = {}\n",
        "  for i in range(1,len(layer_sizes)):\n",
        "      norm_xav=np.sqrt(6)/np.sqrt(layer_sizes[i]+layer_sizes[i-1])\n",
        "      params[\"w\"+str(i)]=np.random.randn(layer_sizes[i],layer_sizes[i-1])*norm_xav\n",
        "      params[\"b\"+str(i)]=np.zeros((layer_sizes[i],1))\n",
        "  \n",
        "  return params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nUVnJQfuUHqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Random(layer_sizes):\n",
        "  params = {}\n",
        "  for i in range(1,len(layer_sizes)):\n",
        "      params[\"w\"+str(i)]=0.01*np.random.randn(layer_sizes[i],layer_sizes[i-1])\n",
        "      params[\"b\"+str(i)]=0.01*np.random.randn(layer_sizes[i],1)\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "1sRkD2gu46dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Activation Functions </h4>"
      ],
      "metadata": {
        "id": "I7VIhXa56Zi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(pre_act):\n",
        "  try:\n",
        "    return (1.0/(1.0+np.exp(-pre_act)))\n",
        "  except:\n",
        "    print(\"error in sigmoid\")"
      ],
      "metadata": {
        "id": "ZM7quY3l6PZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(pre_act):\n",
        "  return (np.tanh(pre_act))\n"
      ],
      "metadata": {
        "id": "dAvWjzlz6RjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(pre_act):\n",
        "  return (np.maximum(0,pre_act))"
      ],
      "metadata": {
        "id": "D8ZpvzQu6TZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  try:\n",
        "    return(np.exp(x)/np.sum(np.exp(x)))\n",
        "  except:\n",
        "    print(\"error in softmax\")"
      ],
      "metadata": {
        "id": "8Vq4OnO96WRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hMEinOgW6XqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Derivatives of Activation Functions </h4>"
      ],
      "metadata": {
        "id": "PZcX9QpD6keQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))"
      ],
      "metadata": {
        "id": "6SgvWXU76reg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh_derivative(x):\n",
        "  return 1.0 -tanh(x)**2\n"
      ],
      "metadata": {
        "id": "EFMQdq_b6rcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(x):\n",
        "  return 1. * (x>0)"
      ],
      "metadata": {
        "id": "bUXYoFkF6rZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_derivative(x):\n",
        "  return softmax(x) * (1-softmax(x))"
      ],
      "metadata": {
        "id": "Up46ku0y6rRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative(A, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    return sigmoid_derivative(A)\n",
        "  elif activation == \"tanh\":\n",
        "    return tanh_derivative(A)\n",
        "  elif activation == \"relu\":\n",
        "    return relu_derivative(A)\n"
      ],
      "metadata": {
        "id": "rHgm-ZbS6OLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Loss Functions</h4>"
      ],
      "metadata": {
        "id": "56KNthDl67G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y, y_hat):\n",
        "  error = np.sum(((y - y_hat)**2) / (2 * len(y)))\n",
        "  return error"
      ],
      "metadata": {
        "id": "PRnQH-867Ayy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossEntropy(y, y_hat):\n",
        "  error = - np.sum( np.multiply(y , np.log(y_hat)))/len(y)\n",
        "  return error"
      ],
      "metadata": {
        "id": "YP4QzvRF7Xw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculating loss \n",
        "def loss_calc(loss_name, y, y_hat, lambd, layer_sizes, parameters):\n",
        "  error=0\n",
        "  if(loss_name == \"squared_loss\"):\n",
        "    error=MSE(y, y_hat)\n",
        "  elif(loss_name == \"cross_entropy\"):\n",
        "    error=CrossEntropy(y, y_hat)\n",
        "    #error = -np.sum(np.sum(y_t*np.log(y_hat)))\n",
        "\n",
        "  #For L2 Regularization\n",
        "  regularized_error = 0.0\n",
        "  for i in range(len(layer_sizes)-1, 0, -1):\n",
        "    regularized_error += (np.sum(parameters[\"w\"+str(i)]))**2\n",
        "  regularized_error = error + ((lambd/(2*len(y)))*(regularized_error))\n",
        "\n",
        "\n",
        "  return regularized_error"
      ],
      "metadata": {
        "id": "MyC7PPN16-Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Accuracy <h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "j1tkJKxj0QzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(res,y_t):\n",
        "    # res is the vector that comes \n",
        "    acc=0.0\n",
        "    \n",
        "    for x in range(len(res)):\n",
        "      if(res[x].argmax()==y_t[x].argmax()):\n",
        "        acc+=1\n",
        "    acc=acc/len(y_t)\n",
        "    return(acc*100)"
      ],
      "metadata": {
        "id": "H-FBfniHt5I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_test_accuracy(y_pred,y_t):\n",
        "  acc=0.0\n",
        "  for i in range(len(y_pred)):\n",
        "    if(y_pred[i]==y_t[i]):\n",
        "      acc+=1\n",
        "  acc=acc/len(y_t)\n",
        "  return(acc*100)"
      ],
      "metadata": {
        "id": "Bghjsd1F6-QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Initialization of Neural Network</h2>"
      ],
      "metadata": {
        "id": "RvRGfWa078JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_init(layer_sizes, init_type = \"random\"):\n",
        "  # Layer Sizes denotes the number of neurons per layer\n",
        "  # 784 is for the input layer. \n",
        "  # 32 is for the hidden layers. \n",
        "  # 10 is for the output layers\n",
        "\n",
        "  # initializing parameters for the neural network, \n",
        "  params={}\n",
        "  if(init_type==\"xavier\"):\n",
        "    params = Xavier(layer_sizes)\n",
        "\n",
        "  elif(init_type==\"random\"):\n",
        "    params = Random(layer_sizes)\n",
        "\n",
        "  else:\n",
        "    print(\"Enter a valid weight initilization type\")\n",
        "\n",
        "  return params\n"
      ],
      "metadata": {
        "id": "2ZICexToBSP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Forward Propagation</h2>"
      ],
      "metadata": {
        "id": "Noyewh3N8QGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X,y,params,active,layer_sizes):\n",
        "  \n",
        "  # Extracting only the image data not the label for the image data\n",
        "  out=copy.deepcopy(X)\n",
        "  out=out.reshape(-1,1)\n",
        "  \n",
        "  #These are stored just to make it easy to keep track of the indices along with layers.\n",
        "  h=[out] # To save the activations for each neuron in a layer\n",
        "  a=[out] # To save the preactivation for each neuron in a layer\n",
        "\n",
        "  if(active==\"sigmoid\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=sigmoid(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"tanh\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=tanh(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"relu\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=relu(out)\n",
        "      h.append(post_a)       \n",
        "  else:\n",
        "    print(\"Enter a valid activation function\") \n",
        "\n",
        "  # Final step for forward propagation, using softmax.\n",
        "  weights=params[\"w\"+str(len(layer_sizes)-1)]\n",
        "  biases=params[\"b\"+str(len(layer_sizes)-1)]\n",
        "  \n",
        "  out=np.dot(weights,h[len(layer_sizes)-2])+biases\n",
        "  a.append(out)\n",
        "  y_hat=softmax(out)\n",
        "  h.append(y_hat)\n",
        "  \n",
        "  \n",
        "  #in h we  are storing values for layers right from input till output\n",
        "  #h0 is input\n",
        "  #in a we are storing values for layers right from input till output\n",
        "  #a0 is input\n",
        "\n",
        "  return h,a,y_hat"
      ],
      "metadata": {
        "id": "8VaQdyqbrqsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Back Propagation</h2>"
      ],
      "metadata": {
        "id": "wmjefdiK8cxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(y, y_hat, h, a, params, loss_type, layer_sizes, activation):\n",
        "  \n",
        "  #here both y_hat and y are assumed to be column vectors\n",
        "\n",
        "\n",
        "\n",
        "  grad = {}\n",
        "\n",
        "  if loss_type == \"squared_loss\":\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = (y_hat - y)\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = (y_hat - y) * softmax_derivative(a[len(layer_sizes)-1])\n",
        "\n",
        "  elif loss_type == 'cross_entropy':\n",
        "    #Here actually it should be one hot vector. But y does the same job\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = -(y-y_hat)\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = -(y/y_hat)\n",
        "\n",
        "  for i in range(len(layer_sizes)-1, 0, -1 ):\n",
        "    #print(i)\n",
        "    #Not considering L2 Regularization here. Instead will cumulate in the update section\n",
        "    grad[\"dw\"+str(i)] = np.dot(grad[\"da\"+str(i)], np.transpose(h[i-1]))\n",
        "    grad[\"db\"+str(i)] = grad[\"da\"+str(i)]\n",
        "\n",
        "    if i > 1:\n",
        "      grad[\"dh\"+str(i-1)] = np.dot(np.transpose(params[\"w\"+str(i)]), grad[\"da\"+str(i)])\n",
        "      grad[\"da\"+str(i-1)] = np.multiply(grad[\"dh\" + str(i-1)], derivative(a[i-1],activation))\n",
        " \n",
        "  return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "kAQEfBMIFfW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate gradients, batchwise"
      ],
      "metadata": {
        "id": "cYFgtV7ZIOCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function):\n",
        "  grads={}\n",
        "  grads.clear()\n",
        "  #iterate over all the points in the current batch\n",
        "  for j in range(len(X)):\n",
        "    y = np.reshape(Y[j], (-1,1))\n",
        "    #Feed forward the data point\n",
        "    h,a,y_hat = forward_prop(X[j], y, parameters, activation, layers)\n",
        "    #backpropagate the error.\n",
        "    new_grads = back_prop(y,y_hat, h,a, parameters, loss_function, layers, activation)\n",
        "    #keep collecting the gradients for all the data (since vanilla GD)\n",
        "    if j == 0:\n",
        "      grads = copy.deepcopy(new_grads)\n",
        "    else:\n",
        "      for k in range(len(layers)-1,0,-1):\n",
        "        grads[\"dw\"+str(k)] += new_grads[\"dw\"+str(k)]\n",
        "        grads[\"db\"+str(k)] += new_grads[\"db\"+str(k)]\n",
        "  \n",
        "  return grads"
      ],
      "metadata": {
        "id": "CJXUB3SdINv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Different Optimization Functions</h2>"
      ],
      "metadata": {
        "id": "JhnoYTOQ8g9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Mini Batch Gradient Descent </h3>"
      ],
      "metadata": {
        "id": "KKHa2-Fh8lye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gd(X_train, y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters):\n",
        "  #parameters = nn_init(layers, 'random')\n",
        "  \n",
        "  grads={}\n",
        "  \n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads = grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "    \n",
        "      #Updating the parameters once every one batch\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - (eta * grads[\"dw\"+str(j)])\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - (eta * grads[\"db\"+str(j)])\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "H7a01Svldakv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Momentum Based Gradient Descent </h3>"
      ],
      "metadata": {
        "id": "yPiDf0vS8vsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gd(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "  #parameters = nn_init(layers, 'random')\n",
        "  \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  gamma = 0.9 #Not treating this as a hyperparameter\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "      #Storing the update history for each parameter.\n",
        "      if i == 0 :\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = eta*grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = eta*grads[\"db\"+str(j)]\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (gamma*update_history[\"w\"+str(j)]) + (eta*grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (gamma*update_history[\"b\"+str(j)]) + (eta*grads[\"db\"+str(j)])\n",
        "\n",
        "    \n",
        "      #Updating the parameters once every one batch with the update_history\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "NHgO8wzQ8-qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Nesterov Accelerated Gradient Descent</h3>"
      ],
      "metadata": {
        "id": "zXL670qB879E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gd(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        " \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  param_lookahead = {}\n",
        "  gamma = 0.9 #not treating this as a hyperparameter.\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      #If it is the first batch, we still dont have the previous history.\n",
        "      #So, lookahead will be same as the current parameters\n",
        "      if i==0:\n",
        "        param_lookahead = copy.deepcopy(parameters)\n",
        "      \n",
        "      #If its not the first batch then we calculate lookahead according to\n",
        "      #the formula.\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          param_lookahead['w'+str(j)] = parameters['w'+str(j)] + (gamma*update_history[\"w\"+str(j)])\n",
        "                                                                  \n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,param_lookahead,activation,layers,loss_function)\n",
        "      \n",
        "      #Storing the update history for each parameter.\n",
        "\n",
        "      #If its the first batch, we dont have any update history yet. So, it will\n",
        "      #be same as the eta*gradients\n",
        "      if i == 0 :\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = eta*grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = eta*grads[\"db\"+str(j)]\n",
        "      \n",
        "      #If its not the first batch, we cumulate the update history as per the \n",
        "      #formula.\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (gamma*update_history[\"w\"+str(j)]) + (eta*grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (gamma*update_history[\"b\"+str(j)]) + (eta*grads[\"db\"+str(j)])\n",
        "\n",
        "    \n",
        "      #Updating the parameters once every one batch with the update_history\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "WEglGxnFE3Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> RMSprop</h3>"
      ],
      "metadata": {
        "id": "xNB0DGgD9Ck7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for rmsprop***************\n",
        "def rmsprop(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  \n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta = 0.9 \n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "   \n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "        \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "        v[\"w\"+str(iq)]=beta*v[\"w\"+str(iq)]+(1-beta)*grads[\"dw\"+str(iq)]**2\n",
        "        v[\"b\"+str(iq)]=beta*v[\"b\"+str(iq)]+(1-beta)*grads[\"db\"+str(iq)]**2\n",
        "          \n",
        "        update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(v[\"w\"+str(iq)]+epsilon)),grads[\"dw\"+str(iq)])\n",
        "        update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(v[\"b\"+str(iq)]+epsilon)),grads[\"db\"+str(iq)])\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22wMGB7d9Cwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Adam </h3>"
      ],
      "metadata": {
        "id": "31e2lLRk9YSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for adam***************\n",
        "def adam(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "  m={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing m \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    m[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    m[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta1 = 0.9 \n",
        "  beta2=0.999\n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "      grads.clear()\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "          m[\"w\"+str(iq)]=beta1*m[\"w\"+str(iq)]+(1-beta1)*grads[\"dw\"+str(iq)]\n",
        "          m[\"b\"+str(iq)]=beta1*m[\"b\"+str(iq)]+(1-beta1)*grads[\"db\"+str(iq)]\n",
        "          \n",
        "          v[\"w\"+str(iq)]=beta2*v[\"w\"+str(iq)]+(1-beta2)*(grads[\"dw\"+str(iq)])**2\n",
        "          v[\"b\"+str(iq)]=beta2*v[\"b\"+str(iq)]+(1-beta2)*(grads[\"db\"+str(iq)])**2\n",
        "\n",
        "          # Bias Correction:\n",
        "          # calculating mt_hat and vt_hat for weights and biases \n",
        "          mw_hat=m[\"w\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "          mb_hat=m[\"b\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "\n",
        "          vw_hat=v[\"w\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          vb_hat=v[\"b\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          \n",
        "          update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vw_hat+epsilon)),mw_hat)\n",
        "          update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vb_hat+epsilon)),mb_hat)\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "          parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "          parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8S6l8vdT9csw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>NAdam</h3>"
      ],
      "metadata": {
        "id": "wHNnGITB9gvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for nadam***************\n",
        "\n",
        "def nadam(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "  m={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  \n",
        "\n",
        "\n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing m \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    m[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    m[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta1 = 0.9 \n",
        "  beta2=0.999\n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        " \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "          m[\"w\"+str(iq)]=beta1*m[\"w\"+str(iq)]+(1-beta1)*grads[\"dw\"+str(iq)]\n",
        "          m[\"b\"+str(iq)]=beta1*m[\"b\"+str(iq)]+(1-beta1)*grads[\"db\"+str(iq)]\n",
        "          \n",
        "          v[\"w\"+str(iq)]=beta2*v[\"w\"+str(iq)]+(1-beta2)*(grads[\"dw\"+str(iq)])**2\n",
        "          v[\"b\"+str(iq)]=beta2*v[\"b\"+str(iq)]+(1-beta2)*(grads[\"db\"+str(iq)])**2\n",
        "\n",
        "          # Bias Correction:\n",
        "          # calculating mt_hat and vt_hat for weights and biases \n",
        "          mw_hat=m[\"w\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "          mb_hat=m[\"b\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "\n",
        "          vw_hat=v[\"w\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          vb_hat=v[\"b\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          \n",
        "          update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vw_hat+epsilon)),(beta1*mw_hat+(1-beta1)*grads[\"dw\"+str(iq)]))*(1/(1-np.power(beta1,t+1)))\n",
        "          update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vb_hat+epsilon)),(beta1*mb_hat+(1-beta1)*grads[\"db\"+str(iq)]))*(1/(1-np.power(beta1,t+1)))\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "          parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "          parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FMxarSbY9ru4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Predict </h2>\n",
        "Function to predict the labels after training the model"
      ],
      "metadata": {
        "id": "6LQFPsXT9u15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train,y_train,parameters,activation,layer_sizes):\n",
        "\n",
        "  '''This function is used to simple take a model parameters\n",
        "      and run the data points using forward prop, and return the outputs \n",
        "      of all the input data points'''\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    h,a,y_hat = forward_prop(X_train[i], y_train[i], parameters, activation, layer_sizes)\n",
        "\n",
        "    #converting y_hat to a 1d array to match with the y\n",
        "    y_hat = y_hat.flatten()\n",
        "    result.append(y_hat)\n",
        "  \n",
        "  return result\n"
      ],
      "metadata": {
        "id": "OVd1N86DZ06N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Fit</h2>\n",
        "Function to train the neural network"
      ],
      "metadata": {
        "id": "8Gahdla9-I9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X_train, y_train, layer_sizes, learning_rate = 0.0001, initialization_type = \"random\", activation_function = \"sigmoid\", loss_function = \"cross_entropy\", mini_batch_Size = 32, max_epochs = 5, lambd = 0, optimization_function = mini_batch_gd): \n",
        "\n",
        "\n",
        "\n",
        "  parameters = nn_init(init_type = initialization_type, layer_sizes = layer_sizes)\n",
        "  parameters, train_errors_list, val_errors_list = optimization_function(X_train, y_train,learning_rate, max_epochs, layer_sizes, mini_batch_Size, lambd, loss_function, activation_function, parameters)\n",
        "  print(train_errors_list)\n",
        "  print(val_errors_list)\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aDYOzjtnt48n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Calling all the functions</h2>"
      ],
      "metadata": {
        "id": "OYdDzZRu-Si9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()"
      ],
      "metadata": {
        "id": "Hsb7BQqXWN-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d4b56d-b34c-424a-975f-931f08c6d1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "  config_defaults = {\n",
        "      'number_hidden_layers': 2,\n",
        "      'number_neurons': 32,\n",
        "      'learning_rate': 0.001,\n",
        "      'initialization_type': \"xavier\",\n",
        "      'activation_function':'sigmoid',\n",
        "      'mini_batch_size' : 64,\n",
        "      'max_epochs': 5,\n",
        "      'lambd': 0,\n",
        "      'optimization_function': \"adam\"\n",
        "      \n",
        "  }\n",
        "\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "\n",
        "  #Forming the layer_sizes i.e., the architecture of our neural network\n",
        "  layer_sizes = [784]\n",
        "  for i in range(config.number_hidden_layers):\n",
        "    layer_sizes = layer_sizes + [config.number_neurons]\n",
        "  layer_sizes  = layer_sizes + [10]\n",
        "\n",
        "  learning_rate = config.learning_rate\n",
        "  initialization_type = config.initialization_type\n",
        "  activation_function = config.activation_function\n",
        "  loss_function = \"cross_entropy\"\n",
        "  mini_batch_size = config.mini_batch_size\n",
        "  max_epochs = config.max_epochs\n",
        "  lambd = config.lambd\n",
        "  opt_fun = config.optimization_function\n",
        "\n",
        "  if opt_fun == \"adam\":\n",
        "    optimization_function = adam\n",
        "  elif opt_fun == \"nadam\":\n",
        "    optimization_function = nadam\n",
        "  elif opt_fun == \"mini_batch_gd\":\n",
        "    optimization_function = mini_batch_gd\n",
        "  elif opt_fun == \"momentum_gd\":\n",
        "    optimization_function = momentum_gd\n",
        "  elif opt_fun == \"nesterov_gd\":\n",
        "    optimization_function = nesterov_gd\n",
        "  elif opt_fun == \"rmsprop\":\n",
        "    optimization_function = rmsprop\n",
        "  else:\n",
        "    print(\"Wrong optimization function\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "\n",
        "  name_run = str(learning_rate) + \"_\" + initialization_type[0] + \"_\" + \\\n",
        "  activation_function[0] + \"_\" + str(mini_batch_size) + \"_\" + str(max_epochs) + \\\n",
        "  \"_\" + str(lambd) + \"_\" + opt_fun[:4]\n",
        "\n",
        "  wandb.run.name = name_run\n",
        "\n",
        "  parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "  \n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bLaUCuD_Jsxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"learning_rate\":{\n",
        "       'values': [0.001, 0.0001]\n",
        "    },\n",
        "\n",
        "    \"number_hidden_layers\": {\n",
        "        'values' : [3, 4, 5]\n",
        "    },\n",
        "\n",
        "    \"number_neurons\": {\n",
        "       'values': [32, 64, 128]\n",
        "    },\n",
        "\n",
        "    \"initialization_type\": {\n",
        "        'values' : [\"xavier\", \"random\"]\n",
        "    },\n",
        "\n",
        "    \"activation_function\": {\n",
        "        'values': [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "    },\n",
        "\n",
        "    \"mini_batch_size\": {\n",
        "        'values': [16,32,64,128]\n",
        "    },\n",
        "\n",
        "    \"max_epochs\": {\n",
        "        'values': [5, 10, 20]\n",
        "    },\n",
        "\n",
        "    \"lambd\": {\n",
        "        'values': [0, 0.0005, 0.5]\n",
        "    },\n",
        "\n",
        "    \"optimization_function\": {\n",
        "        'values': [\"mini_batch_gd\", \"momentum_gd\", \"nesterov_gd\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' :{\n",
        "        'name': 'Validation_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': hyperparameters\n",
        "}"
      ],
      "metadata": {
        "id": "JBZOwcGOip9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"safi-vamsi-cs6910\", project=\"Assignment 1\")\n",
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "CPV1eT1ZpN1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZzVShqd0Jsvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qZZN_1tsqGWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dIreqnQdqGUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Comparing Squared loss and Cross Entropy loss</h2>\n"
      ],
      "metadata": {
        "id": "xVJ98-swLSTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare():\n",
        "\n",
        "\n",
        "  config_defaults = {\n",
        "      'number_hidden_layers': 2,\n",
        "      'number_neurons': 32,\n",
        "      'learning_rate': 0.001,\n",
        "      'initialization_type': \"xavier\",\n",
        "      'activation_function':'sigmoid',\n",
        "      'mini_batch_size' : 64,\n",
        "      'max_epochs': 5,\n",
        "      'lambd': 0,\n",
        "      'optimization_function': \"adam\",\n",
        "      'loss_function': \"cross_entropy\"\n",
        "      \n",
        "  }\n",
        "\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "\n",
        "  #Forming the layer_sizes i.e., the architecture of our neural network\n",
        "  layer_sizes = [784]\n",
        "  for i in range(config.number_hidden_layers):\n",
        "    layer_sizes = layer_sizes + [config.number_neurons]\n",
        "  layer_sizes  = layer_sizes + [10]\n",
        "\n",
        "  learning_rate = config.learning_rate\n",
        "  initialization_type = config.initialization_type\n",
        "  activation_function = config.activation_function\n",
        "  loss_function = config.loss_function\n",
        "  mini_batch_size = config.mini_batch_size\n",
        "  max_epochs = config.max_epochs\n",
        "  lambd = config.lambd\n",
        "  opt_fun = config.optimization_function\n",
        "\n",
        "  if opt_fun == \"adam\":\n",
        "    optimization_function = adam\n",
        "  elif opt_fun == \"nadam\":\n",
        "    optimization_function = nadam\n",
        "  elif opt_fun == \"mini_batch_gd\":\n",
        "    optimization_function = mini_batch_gd\n",
        "  elif opt_fun == \"momentum_gd\":\n",
        "    optimization_function = momentum_gd\n",
        "  elif opt_fun == \"nesterov_gd\":\n",
        "    optimization_function = nesterov_gd\n",
        "  elif opt_fun == \"rmsprop\":\n",
        "    optimization_function = rmsprop\n",
        "  else:\n",
        "    print(\"Wrong optimization function\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "\n",
        "  name_run = str(learning_rate) + \"_\" + initialization_type[0] + \"_\" + \\\n",
        "  activation_function[0] + \"_\" + str(mini_batch_size) + \"_\" + str(max_epochs) + \\\n",
        "  \"_\" + str(lambd) + \"_\" + opt_fun[:4]\n",
        "\n",
        "  wandb.run.name = name_run\n",
        "\n",
        "  parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "  \n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()"
      ],
      "metadata": {
        "id": "ZhrLpiWRqGOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"learning_rate\":{\n",
        "       'values': [0.001, 0.0001]\n",
        "    },\n",
        "\n",
        "    \"number_hidden_layers\": {\n",
        "        'values' : [3, 4, 5]\n",
        "    },\n",
        "\n",
        "    \"number_neurons\": {\n",
        "       'values': [32, 64, 128]\n",
        "    },\n",
        "\n",
        "    \"initialization_type\": {\n",
        "        'values' : [\"xavier\", \"random\"]\n",
        "    },\n",
        "\n",
        "    \"activation_function\": {\n",
        "        'values': [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "    },\n",
        "\n",
        "    \"mini_batch_size\": {\n",
        "        'values': [16,32,64]\n",
        "    },\n",
        "\n",
        "    \"max_epochs\": {\n",
        "        'values': [5, 10, 15]\n",
        "    },\n",
        "\n",
        "    \"lambd\": {\n",
        "        'values': [0, 0.0005, 0.5]\n",
        "    },\n",
        "\n",
        "    \"optimization_function\": {\n",
        "        'values': [\"mini_batch_gd\", \"momentum_gd\", \"nesterov_gd\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "    },\n",
        "\n",
        "    \"loss_function\": {\n",
        "        'values': [\"cross_entropy\", \"squared_loss\"]\n",
        "    }\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method' : 'random',\n",
        "    'metric' :{\n",
        "        'name': 'Validation_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': hyperparameters\n",
        "}"
      ],
      "metadata": {
        "id": "1QeeVSeeqGLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"safi-vamsi-cs6910\",project=\"Assignment 1\")\n",
        "wandb.agent(sweep_id, compare, count = 50)"
      ],
      "metadata": {
        "id": "EYOmvodmqGI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Confusion Matrix for Test Dataset"
      ],
      "metadata": {
        "id": "2CrEVrWQPbwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # NOTE: Change these values according to the observations\n",
        "  # made from the parallel coordinate plot from wandb.ai. These are the best \n",
        "  # hyperparameters as observed.\n",
        "  \n",
        "  config_final_test = {\n",
        "      'number_hidden_layers': 5,\n",
        "      'number_neurons': 32,\n",
        "      'learning_rate': 0.0001,\n",
        "      'initialization_type': \"xavier\",\n",
        "      'activation_function':'relu',\n",
        "      'mini_batch_size' : 32,\n",
        "      'max_epochs': 20,\n",
        "      'lambd': 0,\n",
        "      'optimization_function': \"rmsprop\",\n",
        "      'loss_function': \"cross_entropy\"\n",
        "      \n",
        "  }\n",
        "\n",
        "  wandb.init(config=config_final_test)\n",
        "  config = wandb.config\n",
        "\n",
        "\n",
        "  #Forming the layer_sizes i.e., the architecture of our neural network\n",
        "  layer_sizes = [784]\n",
        "  for i in range(config.number_hidden_layers):\n",
        "    layer_sizes = layer_sizes + [config.number_neurons]\n",
        "  layer_sizes  = layer_sizes + [10]\n",
        "\n",
        "  learning_rate = config.learning_rate\n",
        "  initialization_type = config.initialization_type\n",
        "  activation_function = config.activation_function\n",
        "  loss_function = config.loss_function\n",
        "  mini_batch_size = config.mini_batch_size\n",
        "  max_epochs = config.max_epochs\n",
        "  lambd = config.lambd\n",
        "  opt_fun = config.optimization_function\n",
        "\n",
        "  if opt_fun == \"adam\":\n",
        "    optimization_function = adam\n",
        "  elif opt_fun == \"nadam\":\n",
        "    optimization_function = nadam\n",
        "  elif opt_fun == \"mini_batch_gd\":\n",
        "    optimization_function = mini_batch_gd\n",
        "  elif opt_fun == \"momentum_gd\":\n",
        "    optimization_function = momentum_gd\n",
        "  elif opt_fun == \"nesterov_gd\":\n",
        "    optimization_function = nesterov_gd\n",
        "  elif opt_fun == \"rmsprop\":\n",
        "    optimization_function = rmsprop\n",
        "  else:\n",
        "    print(\"Wrong optimization function\")\n",
        "    exit()\n",
        "\n",
        "  parameters_for_test = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "  res = predict(X_test,y_test, parameters_for_test, activation_function, layer_sizes)\n",
        "  \n",
        "  # Converting the one hot encoded vectors back to label_id's\n",
        "  y_t=[]\n",
        "  for k in range(len(y_test)):\n",
        "    y_t.append(y_test[k].argmax())\n",
        "\n",
        "  y_pred=[]\n",
        "  for k in range(len(res)):\n",
        "    y_pred.append(res[k].argmax())\n",
        "\n",
        "  test_accuracy=calc_test_accuracy(y_pred,y_t)\n",
        "  wandb.log({\"conf_mat\":wandb.plot.confusion_matrix(preds=y_pred,y_true=y_t,class_names=labels),\"Test Accuracy\": test_accuracy}) \n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "ADKEbRF9qGDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting confusion matrix on local machine :\n",
        "\n",
        "def plot_local_confmat(y_true,y_pred):\n",
        "  labs=np.unique(y_true)\n",
        "  cmat=np.zeros((len(labs),len(labs)))\n",
        "\n",
        "  for i in range(len(labs)):\n",
        "    for j in range(len(labs)):\n",
        "      cmat[i,j]=np.sum((y_true==labs[i])&(y_pred==labs[j]))\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  ax.set_title(\"Confusion matrix for Test dataset\")\n",
        "  sn.heatmap(cmat,annot=True,xticklabels=labels,yticklabels=labels)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "RtPhg15lqF_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u8xg8pCZQ76m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CL7v5BYbQ74l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bwR8ELPvQ72h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DNQzJJCEQ70T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Trials"
      ],
      "metadata": {
        "id": "8bmuT3iRPsCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "layer_sizes = [784,128,128,128,128,10]\n",
        "learning_rate = 0.001\n",
        "initialization_type = \"xavier\"\n",
        "activation_function = \"sigmoid\"\n",
        "loss_function = \"squared_loss\"\n",
        "mini_batch_size = 32\n",
        "max_epochs = 10\n",
        "lambd = 0.0005 \n",
        "optimization_function = nadam\n",
        "\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()\n",
        "\n",
        "parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "res = predict(X_train,y_train, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_train, res, lambd, layer_sizes, parameters)\n",
        "acc= calc_accuracy(res,y_train)\n",
        "print(\"Train loss is :\",err)\n",
        "print(\"Train accuracy:\",acc)\n",
        "\n",
        "res = predict(X_test,y_test, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_test, res, lambd, layer_sizes, parameters)\n",
        "acc= calc_accuracy(res,y_test)\n",
        "print(\"Test loss is :\",err)\n",
        "print(\"Test accuracy:\",acc)"
      ],
      "metadata": {
        "id": "pH8uZIeiRodf",
        "outputId": "e26e8b38-85d8-422d-cce1-758a5a2d45fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [09:43<02:25, 72.97s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-7f67f4b1093e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialization_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimization_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-df6c0e33f7e8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_Size, max_epochs, lambd, optimization_function)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialization_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_errors_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_errors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimization_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_Size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_errors_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_errors_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-6853d221a03a>\u001b[0m in \u001b[0;36mnadam\u001b[0;34m(X_train, y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters)\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_calculate_batchwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0miq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a0e6b9d2f013>\u001b[0m in \u001b[0;36mgrad_calculate_batchwise\u001b[0;34m(X, Y, parameters, activation, layers, loss_function)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#Feed forward the data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#backpropagate the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnew_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-795c78569a97>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(X, y, params, active, layer_sizes)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mbiases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mpost_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "layer_sizes = [784,32,32,10]\n",
        "learning_rate = 0.0001\n",
        "initialization_type = \"xavier\"\n",
        "activation_function = \"sigmoid\"\n",
        "loss_function = \"cross_entropy\"\n",
        "mini_batch_size = 32\n",
        "max_epochs = 20\n",
        "lambd = 0 \n",
        "optimization_function = n_adam\n",
        "\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()\n",
        "\n",
        "parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "res = predict(X_train,y_train, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_train, res, lambd, layer_sizes, parameters)\n",
        "print(\"Train loss is :\",err)\n",
        "\n",
        "\n",
        "res = predict(X_test,y_test, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_test, res, lambd, layer_sizes, parameters)\n",
        "print(\"Test loss is :\",err)\n"
      ],
      "metadata": {
        "id": "O53Um2LQfaxM",
        "outputId": "59ad3af5-4d02-4f37-b5dc-289868a7724c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:39<00:00, 19.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6088209769084224, 0.5523804249973134, 0.5285184353646958, 0.5142271645329473, 0.5042819353625889, 0.4967253705491029, 0.4906372046572024, 0.48552448917878577, 0.4810978723198597, 0.4771753414464648, 0.473636056563522, 0.47039626925518757, 0.4673958813740041, 0.46459051288077574, 0.461946598478084, 0.4594382354892736, 0.45704508862681054, 0.4547509564948635, 0.4525427658327407, 0.4504098500635978]\n",
            "Train loss is : 0.4504098500635978\n",
            "Test loss is : 0.4820588075482052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7hNoGpvKgUkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}