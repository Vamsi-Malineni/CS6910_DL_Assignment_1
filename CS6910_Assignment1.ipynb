{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " CS6910_Assignment1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safikhanSoofiyani/CS6910-Assignment1/blob/main/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries."
      ],
      "metadata": {
        "id": "s_fr1Bcc6eOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ahmgbKIg2f0o"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing wandb"
      ],
      "metadata": {
        "id": "OG-qZnHL7Gyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "id": "VVG9Nfzc7DLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441b4532-f75d-4c61-b970-d25c9b0d3219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 64.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 143 kB 68.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MISSING******* Plotting images via wandb************"
      ],
      "metadata": {
        "id": "-9BOTbyz3rpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing dataset"
      ],
      "metadata": {
        "id": "GU1iINihIbYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepdata():\n",
        "  # Loading data\n",
        "  (train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()\n",
        "\n",
        "  # Defining labels for data\n",
        "  num_classes = 10\n",
        "  labels=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "  print(\"Number of data points in train data (initially) - \", len(train_x))\n",
        "  print(\"Number of data points in test data (initially) - \", len(test_x))\n",
        "\n",
        "\n",
        "  #performing the train-validation split\n",
        "  train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=40)\n",
        "  \n",
        "\n",
        "  print(\"Shape of each image - 28x28\" )\n",
        "  image_shape=train_x.shape[1]*train_x.shape[2]\n",
        "  print(\"shape of each image (1D) - \",image_shape)\n",
        "  \n",
        "  train_image_count=len(train_x)\n",
        "  val_image_count = len(val_x)\n",
        "  test_image_count=len(test_x)\n",
        "  \n",
        "  # Creating a matrix of image data \n",
        "  # each image is represented as a row by flattening the matrix: converting (60000,28,28) tensor to (60000,784) matrix\n",
        "  X_train=np.zeros((train_image_count,image_shape))\n",
        "  X_val=np.zeros((val_image_count,image_shape))\n",
        "  X_test=np.zeros((test_image_count,image_shape))\n",
        "  \n",
        "  # converting the images into grayscale by normalizing\n",
        "  for i in range(train_image_count):\n",
        "    X_train[i]=(copy.deepcopy(train_x[i].flatten()))/255.0 \n",
        "  for i in range(val_image_count):\n",
        "    X_val[i]=(copy.deepcopy(val_x[i].flatten()))/255.0\n",
        "  for i in range(test_image_count):\n",
        "    X_test[i]=(copy.deepcopy(test_x[i].flatten()))/255.0\n",
        "  \n",
        "\n",
        "\n",
        "  #Once hot encoding the label vectors\n",
        "  y_train = np.zeros((train_y.size, 10))\n",
        "  y_train[np.arange(train_y.size), train_y] = 1\n",
        "\n",
        "  y_val = np.zeros((val_y.size, 10))\n",
        "  y_val[np.arange(val_y.size), val_y] = 1\n",
        "\n",
        "  y_test = np.zeros((test_y.size, 10))\n",
        "  y_test[np.arange(test_y.size), test_y] = 1\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  return X_train,X_val,X_test,y_train,y_val,y_test\n",
        "  "
      ],
      "metadata": {
        "id": "uIanRhNjIZiu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "7OI2Z4lEK1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate the functions (to be done later)"
      ],
      "metadata": {
        "id": "nUVnJQfuUHqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_init(layer_sizes,init_type):\n",
        "  # Layer Sizes denotes the number of neurons per layer\n",
        "  # 784 is for the input layer. \n",
        "  # 32 is for the hidden layers. \n",
        "  # 10 is for the output layers\n",
        "\n",
        "  # initializing parameters for the neural network, \n",
        "  params={}\n",
        "  if(init_type==\"xavier\"):\n",
        "    for i in range(1,len(layer_sizes)):\n",
        "      norm_xav=np.sqrt(6)/np.sqrt(layer_sizes[i]+layer_sizes[i-1])\n",
        "      params[\"w\"+str(i)]=np.random.randn(layer_sizes[i],layer_sizes[i-1])*norm_xav\n",
        "      params[\"b\"+str(i)]=np.zeros((layer_sizes[i],1))\n",
        "\n",
        "  elif(init_type==\"random\"):\n",
        "    for i in range(1,len(layer_sizes)):\n",
        "      params[\"w\"+str(i)]=np.random.randn(layer_sizes[i],layer_sizes[i-1])\n",
        "      params[\"b\"+str(i)]=np.random.randn((layer_sizes[i],1))\n",
        "\n",
        "  else:\n",
        "    print(\"Enter a valid weight initilization type\")\n",
        "\n",
        "  return params\n"
      ],
      "metadata": {
        "id": "2ZICexToBSP0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Functions\n",
        "\n",
        "def sigmoid(pre_act):\n",
        "  return (1/(1+np.exp(-pre_act)))\n",
        "\n",
        "def tanh(pre_act):\n",
        "  return (np.tanh(pre_act))\n",
        "\n",
        "def relu(pre_act):\n",
        "  return (np.maximum(0,pre_act))\n",
        "\n",
        "def softmax(x):\n",
        "  return(np.exp(x)/np.sum(np.exp(x)))"
      ],
      "metadata": {
        "id": "KM-1Mhdzhvgk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X,y,params,active,layer_sizes):\n",
        "  \n",
        "  # Extracting only the image data not the label for the image data\n",
        "  out=copy.deepcopy(X)\n",
        "  out=out.reshape(-1,1)\n",
        "  \n",
        "  h=[out] # To save the activations for each neuron in a layer\n",
        "  a=[out] # To save the preactivation for each neuron in a layer\n",
        "\n",
        "  if(active==\"sigmoid\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,out)+biases\n",
        "      a.append(out)\n",
        "      post_a=sigmoid(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"tanh\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,out)+biases\n",
        "      a.append(out)\n",
        "      post_a=tanh(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"relu\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,out)+biases\n",
        "      a.append(out)\n",
        "      post_a=relu(out)\n",
        "      h.append(post_a)       \n",
        "  else:\n",
        "    print(\"Enter a valid activation function\") \n",
        "\n",
        "  # Final step for forward propagation, using softmax.\n",
        "  weights=params[\"w\"+str(len(layer_sizes)-1)]\n",
        "  biases=params[\"b\"+str(len(layer_sizes)-1)]\n",
        "  \n",
        "  out=np.dot(weights,post_a)+biases\n",
        "  a.append(out)\n",
        "  y_hat=softmax(out)\n",
        "  h.append(y_hat)\n",
        "  \n",
        "  \n",
        "  #in h we  are storing values for layers right from input till output\n",
        "  #h0 is input\n",
        "  #in a we are storing values for layers right from input till output\n",
        "  #a0 is input\n",
        "\n",
        "  return h,a,y_hat"
      ],
      "metadata": {
        "id": "8VaQdyqbrqsO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gyAV0FfBmf9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_val,X_test,y_train,y_val,y_test = prepdata()\n",
        "parameters=nn_init(layer_sizes=[3,3,3,2],init_type=\"xavier\")\n",
        "h,a,y_hat = forward_prop(np.array([2,3,5]), [0,1],parameters, \"sigmoid\", [3,3,3,2])\n",
        "\n"
      ],
      "metadata": {
        "id": "LBTwnLGLpcd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef22527f-9a66-4a96-a920-150ff17eec88"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[1])"
      ],
      "metadata": {
        "id": "H6qN_Z8-ILJv",
        "outputId": "9bda560f-b61b-4263-c17c-86584266f86d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "id": "Qyff5W3gyGQT",
        "outputId": "023291f0-0458-4fb5-cf15-614294303738",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[2],\n",
            "       [3],\n",
            "       [5]]), array([[-2.85190181],\n",
            "       [ 6.18249135],\n",
            "       [ 6.07477903]]), array([[-10.70717507],\n",
            "       [  0.63775227],\n",
            "       [-12.47237723]]), array([[ 0.55525001],\n",
            "       [-0.5318536 ]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating loss \n",
        "def loss_calc(name,y_t,y_hat):\n",
        "  error=0\n",
        "  if(name==\"sse\"):\n",
        "    error=np.sum(((y_t-y_hat)**2))\n",
        "  elif(name==\"cross_entropy\"):\n",
        "    error=-1*np.sum(np.multiply(y_t,np.log(y_hat)))\n",
        "\n",
        "  return error\n"
      ],
      "metadata": {
        "id": "199HeO5lisTZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating derivatives of the acitvation functions\n",
        "def derivative(A, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    return sigmoid_der(A)\n",
        "  elif activation == \"tanh\":\n",
        "    return tanh_der(A)\n",
        "  elif activation == \"relu\":\n",
        "    return relu_der(A)\n",
        "\n",
        "\n",
        "def sigmoid_der(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def tanh_der(x):\n",
        "  return 1-tanh(x)**2\n",
        "\n",
        "def relu_der(x):\n",
        "  return 1. * (x>0)\n"
      ],
      "metadata": {
        "id": "gjjBG1delMS0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(y, y_hat, h, a, params, loss_type, layer_sizes, activation):\n",
        "  \n",
        "  #here both y_hat and y are assumed to be column vectors\n",
        "\n",
        "\n",
        "\n",
        "  grad = {}\n",
        "\n",
        "  if loss_type == \"squared_error\":\n",
        "    #waiting\n",
        "    k = 0\n",
        "\n",
        "  elif loss_type == 'cross_entropy':\n",
        "    #Here actually it should be one hot vector. But y does the same job\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = -(y-y_hat)\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = -(y/y_hat)\n",
        "\n",
        "  for i in range(len(layer_sizes)-1, 0, -1 ):\n",
        "    print(i)\n",
        "    grad[\"dw\"+str(i)] = np.dot(grad[\"da\"+str(i)], np.transpose(h[i-1]))\n",
        "    grad[\"db\"+str(i)] = grad[\"da\"+str(i)]\n",
        "\n",
        "    if i > 1:\n",
        "      grad[\"dh\"+str(i-1)] = np.dot(np.transpose(params[\"w\"+str(i)]), grad[\"da\"+str(i)])\n",
        "      grad[\"da\"+str(i-1)] = np.multiply(grad[\"dh\" + str(i-1)], derivative(a[i-1],activation))\n",
        " \n",
        "  return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "kAQEfBMIFfW0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = np.reshape(y_train[1], (-1,1))\n",
        "print(k)\n",
        "print(y_train[1])"
      ],
      "metadata": {
        "id": "rr_W4dH0eKJW",
        "outputId": "c10a22bc-b7ff-43c5-e48a-4f915313d5fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_gd(X_train,y_train, ):\n",
        "  parameters = nn_init([784,32,32,10], 'random')\n",
        "  eta = 0.1\n",
        "  max_epochs = 10000\n",
        "  for t in range(1000):\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "      y = np.reshape(y_train[i], (-1,1))\n",
        "      h,a,y_hat = forward_prop(X_train[i], y, parameters, \"sigmoid\", [784,32,32,10])\n",
        "      new_grads = back_prop(y,y_hat, h,a, parameters, \"cross_entropy\", [784,32,32,10], \"sigmoid\")\n",
        "      if i == 0:\n",
        "        grads = copy.deepcopy(new_grads)\n",
        "      #adding the gradients\n",
        "      for j in range(3,0,-1):\n",
        "        grads[\"dw\"+str(i)] += new_grads[\"dw\"+str(i)]\n",
        "        grads[\"db\"+str(i)] += new_grads[\"db\"+str(i)]\n",
        "        grads[\"da\"+str(i)] += new_grads[\"da\"+str(i)]\n",
        "        grads[\"dh\"+str(i)] += new_grads[\"dh\"+str(i)]\n",
        "\n",
        "\n",
        "\n",
        "    for j in range(3,0,-1):\n",
        "      parameters[\"w\"+str(j)] = parameters[\"w\"+str(j)] - (eta * grads[\"dw\"+str(j)])\n",
        "      parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - (eta * grads[\"db\"+str(j)])\n"
      ],
      "metadata": {
        "id": "H7a01Svldakv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}