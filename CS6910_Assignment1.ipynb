{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " CS6910_Assignment1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safikhanSoofiyani/CS6910-Assignment1/blob/main/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries."
      ],
      "metadata": {
        "id": "s_fr1Bcc6eOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ahmgbKIg2f0o"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy \n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing wandb"
      ],
      "metadata": {
        "id": "OG-qZnHL7Gyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "id": "VVG9Nfzc7DLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441b4532-f75d-4c61-b970-d25c9b0d3219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 64.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 143 kB 68.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MISSING******* Plotting images via wandb************"
      ],
      "metadata": {
        "id": "-9BOTbyz3rpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing dataset"
      ],
      "metadata": {
        "id": "GU1iINihIbYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepdata():\n",
        "  # Loading data\n",
        "  (train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()\n",
        "\n",
        "  # Defining labels for data\n",
        "  num_classes = 10\n",
        "  labels=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "  print(\"Number of data points in train data (initially) - \", len(train_x))\n",
        "  print(\"Number of data points in test data (initially) - \", len(test_x))\n",
        "\n",
        "\n",
        "  #performing the train-validation split\n",
        "  train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=40)\n",
        "  \n",
        "\n",
        "  print(\"Shape of each image - 28x28\" )\n",
        "  image_shape=train_x.shape[1]*train_x.shape[2]\n",
        "  print(\"shape of each image (1D) - \",image_shape)\n",
        "  \n",
        "  train_image_count=len(train_x)\n",
        "  val_image_count = len(val_x)\n",
        "  test_image_count=len(test_x)\n",
        "  \n",
        "  # Creating a matrix of image data \n",
        "  # each image is represented as a row by flattening the matrix: converting (60000,28,28) tensor to (60000,784) matrix\n",
        "  X_train=np.zeros((train_image_count,image_shape))\n",
        "  X_val=np.zeros((val_image_count,image_shape))\n",
        "  X_test=np.zeros((test_image_count,image_shape))\n",
        "  \n",
        "  # converting the images into grayscale by normalizing\n",
        "  for i in range(train_image_count):\n",
        "    X_train[i]=(copy.deepcopy(train_x[i].flatten()))/255.0 \n",
        "  for i in range(val_image_count):\n",
        "    X_val[i]=(copy.deepcopy(val_x[i].flatten()))/255.0\n",
        "  for i in range(test_image_count):\n",
        "    X_test[i]=(copy.deepcopy(test_x[i].flatten()))/255.0\n",
        "  \n",
        "\n",
        "\n",
        "  #Once hot encoding the label vectors\n",
        "  y_train = np.zeros((train_y.size, 10))\n",
        "  y_train[np.arange(train_y.size), train_y] = 1\n",
        "\n",
        "  y_val = np.zeros((val_y.size, 10))\n",
        "  y_val[np.arange(val_y.size), val_y] = 1\n",
        "\n",
        "  y_test = np.zeros((test_y.size, 10))\n",
        "  y_test[np.arange(test_y.size), test_y] = 1\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  return X_train,X_val,X_test,y_train,y_val,y_test\n",
        "  "
      ],
      "metadata": {
        "id": "uIanRhNjIZiu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "7OI2Z4lEK1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate the functions (to be done later)"
      ],
      "metadata": {
        "id": "nUVnJQfuUHqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_init(layer_sizes,init_type):\n",
        "  # Layer Sizes denotes the number of neurons per layer\n",
        "  # 784 is for the input layer. \n",
        "  # 32 is for the hidden layers. \n",
        "  # 10 is for the output layers\n",
        "\n",
        "  # initializing parameters for the neural network, \n",
        "  params={}\n",
        "  if(init_type==\"xavier\"):\n",
        "    for i in range(1,len(layer_sizes)):\n",
        "      norm_xav=np.sqrt(6)/np.sqrt(layer_sizes[i]+layer_sizes[i-1])\n",
        "      params[\"w\"+str(i)]=np.random.randn(layer_sizes[i],layer_sizes[i-1])*norm_xav\n",
        "      params[\"b\"+str(i)]=np.zeros((layer_sizes[i],1))\n",
        "\n",
        "  elif(init_type==\"random\"):\n",
        "    for i in range(1,len(layer_sizes)):\n",
        "      params[\"w\"+str(i)]=np.random.randn(layer_sizes[i],layer_sizes[i-1])\n",
        "      params[\"b\"+str(i)]=np.random.randn(layer_sizes[i],1)\n",
        "\n",
        "  else:\n",
        "    print(\"Enter a valid weight initilization type\")\n",
        "\n",
        "  return params\n"
      ],
      "metadata": {
        "id": "2ZICexToBSP0"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Functions\n",
        "\n",
        "def sigmoid(pre_act):\n",
        "  try:\n",
        "    return (1/(1+np.exp(-pre_act)))\n",
        "  except:\n",
        "    print(\"error in sigmoid\")\n",
        "\n",
        "def tanh(pre_act):\n",
        "  return (np.tanh(pre_act))\n",
        "\n",
        "def relu(pre_act):\n",
        "  return (np.maximum(0,pre_act))\n",
        "\n",
        "def softmax(x):\n",
        "  try:\n",
        "    return(np.exp(x)/np.sum(np.exp(x)))\n",
        "  except:\n",
        "    print(\"error in softmax\")"
      ],
      "metadata": {
        "id": "KM-1Mhdzhvgk"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X,y,params,active,layer_sizes):\n",
        "  \n",
        "  # Extracting only the image data not the label for the image data\n",
        "  out=copy.deepcopy(X)\n",
        "  out=out.reshape(-1,1)\n",
        "  \n",
        "  #These are stored just to make it easy to keep track of the indices along with layers.\n",
        "  h=[out] # To save the activations for each neuron in a layer\n",
        "  a=[out] # To save the preactivation for each neuron in a layer\n",
        "\n",
        "  if(active==\"sigmoid\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,out)+biases\n",
        "      a.append(out)\n",
        "      post_a=sigmoid(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"tanh\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,out)+biases\n",
        "      a.append(out)\n",
        "      post_a=tanh(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"relu\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,out)+biases\n",
        "      a.append(out)\n",
        "      post_a=relu(out)\n",
        "      h.append(post_a)       \n",
        "  else:\n",
        "    print(\"Enter a valid activation function\") \n",
        "\n",
        "  # Final step for forward propagation, using softmax.\n",
        "  weights=params[\"w\"+str(len(layer_sizes)-1)]\n",
        "  biases=params[\"b\"+str(len(layer_sizes)-1)]\n",
        "  \n",
        "  out=np.dot(weights,post_a)+biases\n",
        "  a.append(out)\n",
        "  y_hat=softmax(out)\n",
        "  h.append(y_hat)\n",
        "  \n",
        "  \n",
        "  #in h we  are storing values for layers right from input till output\n",
        "  #h0 is input\n",
        "  #in a we are storing values for layers right from input till output\n",
        "  #a0 is input\n",
        "\n",
        "  return h,a,y_hat"
      ],
      "metadata": {
        "id": "8VaQdyqbrqsO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gyAV0FfBmf9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_val,X_test,y_train,y_val,y_test = prepdata()\n",
        "parameters=nn_init(layer_sizes=[3,3,3,2],init_type=\"xavier\")\n",
        "h,a,y_hat = forward_prop(np.array([2,3,5]), [0,1],parameters, \"sigmoid\", [3,3,3,2])\n",
        "\n"
      ],
      "metadata": {
        "id": "LBTwnLGLpcd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f1bb10-a31a-4257-d6f4-6a6b39371b90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[1])"
      ],
      "metadata": {
        "id": "H6qN_Z8-ILJv",
        "outputId": "fe27d6bf-d655-410c-b8b5-78d836dc794d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "id": "Qyff5W3gyGQT",
        "outputId": "bb8497b4-5801-479d-81e8-7f2d81f5416c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[2],\n",
            "       [3],\n",
            "       [5]]), array([[-4.485352  ],\n",
            "       [ 3.6082013 ],\n",
            "       [12.06190275]]), array([[ -7.82075448],\n",
            "       [-13.74612755],\n",
            "       [ 16.03472001]]), array([[1.05009371],\n",
            "       [0.00955889]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating loss \n",
        "def loss_calc(name,y_t,y_hat):\n",
        "  error=0\n",
        "  if(name==\"sse\"):\n",
        "    error=np.sum(((y_t-y_hat)**2))\n",
        "  elif(name==\"cross_entropy\"):\n",
        "    #error=-1*np.sum(np.multiply(y_t,np.log(y_hat)))\n",
        "    error = -np.sum(np.sum(y_t*np.log(y_hat)))\n",
        "\n",
        "  return error\n"
      ],
      "metadata": {
        "id": "199HeO5lisTZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating derivatives of the acitvation functions\n",
        "def derivative(A, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    return sigmoid_der(A)\n",
        "  elif activation == \"tanh\":\n",
        "    return tanh_der(A)\n",
        "  elif activation == \"relu\":\n",
        "    return relu_der(A)\n",
        "\n",
        "\n",
        "def sigmoid_der(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def tanh_der(x):\n",
        "  return 1-tanh(x)**2\n",
        "\n",
        "def relu_der(x):\n",
        "  return 1. * (x>0)\n"
      ],
      "metadata": {
        "id": "gjjBG1delMS0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(y, y_hat, h, a, params, loss_type, layer_sizes, activation):\n",
        "  \n",
        "  #here both y_hat and y are assumed to be column vectors\n",
        "\n",
        "\n",
        "\n",
        "  grad = {}\n",
        "\n",
        "  if loss_type == \"squared_error\":\n",
        "    #waiting\n",
        "    k = 0\n",
        "\n",
        "  elif loss_type == 'cross_entropy':\n",
        "    #Here actually it should be one hot vector. But y does the same job\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = -(y-y_hat)\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = -(y/y_hat)\n",
        "\n",
        "  for i in range(len(layer_sizes)-1, 0, -1 ):\n",
        "    #print(i)\n",
        "    grad[\"dw\"+str(i)] = np.dot(grad[\"da\"+str(i)], np.transpose(h[i-1]))\n",
        "    grad[\"db\"+str(i)] = grad[\"da\"+str(i)]\n",
        "\n",
        "    if i > 1:\n",
        "      grad[\"dh\"+str(i-1)] = np.dot(np.transpose(params[\"w\"+str(i)]), grad[\"da\"+str(i)])\n",
        "      grad[\"da\"+str(i-1)] = np.multiply(grad[\"dh\" + str(i-1)], derivative(a[i-1],activation))\n",
        " \n",
        "  return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "kAQEfBMIFfW0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_gd(X_train,y_train):\n",
        "  parameters = nn_init([784,32,32,10], 'random')\n",
        "  eta = 0.1\n",
        "  max_epochs = 1\n",
        "  layers = [784,32,32,10]\n",
        "  grads={}\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "    grads.clear()\n",
        "\n",
        "    #iterate over all the data points\n",
        "    for i in range(len(X_train)):\n",
        "\n",
        "      y = np.reshape(y_train[i], (-1,1))\n",
        "\n",
        "      #Feed forward the data point\n",
        "      h,a,y_hat = forward_prop(X_train[i], y, parameters, \"sigmoid\", layers)\n",
        "\n",
        "      #backpropagate the error.\n",
        "      new_grads = back_prop(y,y_hat, h,a, parameters, \"cross_entropy\", layers, \"sigmoid\")\n",
        "\n",
        "      #keep collecting the gradients for all the data (since vanilla GD)\n",
        "      if i == 0:\n",
        "        grads = copy.deepcopy(new_grads)\n",
        "      else:\n",
        "        #adding the gradients\n",
        "        for j in range(3,0,-1):\n",
        "          grads[\"dw\"+str(j)] += new_grads[\"dw\"+str(j)]\n",
        "          grads[\"db\"+str(j)] += new_grads[\"db\"+str(j)]\n",
        "          #grads[\"da\"+str(i)] += new_grads[\"da\"+str(i)]\n",
        "          #grads[\"dh\"+str(i)] += new_grads[\"dh\"+str(i)]\n",
        "        \n",
        "    \n",
        "    #Updating the parameters once every one epoch\n",
        "    for j in range(3,0,-1):\n",
        "      parameters[\"w\"+str(j)] = parameters[\"w\"+str(j)] - (eta * grads[\"dw\"+str(j)])\n",
        "      parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - (eta * grads[\"db\"+str(j)])\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "H7a01Svldakv"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train,y_train,parameters,active,layer_sizes):\n",
        "  result = []\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    h,a,y_hat = forward_prop(X_train[i], y_train[i], parameters, \"sigmoid\", [784,32,32,10])\n",
        "\n",
        "    #converting y_hat to a 1d array to match with the y\n",
        "    y_hat = y_hat.flatten()\n",
        "    result.append(y_hat)\n",
        "  \n",
        "  return result\n"
      ],
      "metadata": {
        "id": "OVd1N86DZ06N"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = vanilla_gd(X_train,y_train)\n",
        "\n",
        "res = predict(X_train,y_train,params, \"sigmoid\", [784,32,32,10])\n",
        "\n",
        "err = loss_calc(\"cross_entropy\", y_train,res)\n",
        "\n",
        "\n",
        "print(err)"
      ],
      "metadata": {
        "id": "U7qL7Kz7Z0h0",
        "outputId": "f6abee10-4f7a-4942-ce09-22cc2eb08ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:14<00:00, 14.78s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d1\n",
            "d2\n",
            "d3\n",
            "nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in multiply\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res[100])"
      ],
      "metadata": {
        "id": "0C-6RUerCHRL",
        "outputId": "dea8abe8-21e8-4303-ea40-1d3ddb0e3e76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nan nan nan nan nan  0.  0. nan nan nan]\n"
          ]
        }
      ]
    }
  ]
}