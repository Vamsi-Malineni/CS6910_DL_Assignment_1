{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " CS6910_Assignment1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a958be177adc44e0b87122b1f0b7795c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0b6e86abb4d452a9c47a92e24b26ceb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a9f9fa5105a4cbda85b1752ae15fbfc",
              "IPY_MODEL_ee9990dd48a747f4b47c6ac280ee8e91"
            ]
          }
        },
        "b0b6e86abb4d452a9c47a92e24b26ceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a9f9fa5105a4cbda85b1752ae15fbfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_adad95d14655464b8bcb68caa8f4ebef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_741a2c058bc54fec92833b46cd856630"
          }
        },
        "ee9990dd48a747f4b47c6ac280ee8e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fba0f6ca9f744d0c8d0e37bb9dade147",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ca20cc84df0407db3f2086374452744"
          }
        },
        "adad95d14655464b8bcb68caa8f4ebef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "741a2c058bc54fec92833b46cd856630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fba0f6ca9f744d0c8d0e37bb9dade147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ca20cc84df0407db3f2086374452744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebb5a859a23746f584fb88c86fc3222f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_091e88dfd72d424587b9d10df4fab4d0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eb659b500f9f4019bf822c9de7cf944e",
              "IPY_MODEL_f21fe5e25d5b41a6a2cd2f2007056ef8"
            ]
          }
        },
        "091e88dfd72d424587b9d10df4fab4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb659b500f9f4019bf822c9de7cf944e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_a05c9bdeda2a48b39f64b81a76475ce2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db33e8feecd347b9a732e9bac8d67d73"
          }
        },
        "f21fe5e25d5b41a6a2cd2f2007056ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd81e77088154ba8926a87f69afcbcc6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04c614f49f37416bb4d109b8ddd5d5e1"
          }
        },
        "a05c9bdeda2a48b39f64b81a76475ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db33e8feecd347b9a732e9bac8d67d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd81e77088154ba8926a87f69afcbcc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04c614f49f37416bb4d109b8ddd5d5e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safikhanSoofiyani/CS6910-Assignment1/blob/main/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Feed Forward Neural Network</h1>\n",
        "\n",
        "Work Done by:<br>\n",
        "<ul> \n",
        "<li>Mohammed Safi Ur Rahman Khan - CS21M035 </li>\n",
        "<li>Vamsi Sai Krishna Malineni  - OE20S302 </li>"
      ],
      "metadata": {
        "id": "R5q7bg9s5L8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Importing necessary libraries.</h3>"
      ],
      "metadata": {
        "id": "s_fr1Bcc6eOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ahmgbKIg2f0o"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy \n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various Links used till now:\n",
        "<ul>\n",
        "<li> L2 Regularization - https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd\n",
        "</ul>"
      ],
      "metadata": {
        "id": "M3hgrdYAmz7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing wandb"
      ],
      "metadata": {
        "id": "OG-qZnHL7Gyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "id": "VVG9Nfzc7DLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7e761b-b320-4093-a176-cf6dd12a3e6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 32.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144 kB 53.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 14.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparing dataset</h3>"
      ],
      "metadata": {
        "id": "GU1iINihIbYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data():\n",
        "\n",
        "  '''This function is used to load the data, define the class labels, performing\n",
        "      the train-test-validation split, normalizing the data, flattening each data\n",
        "      point, converting the class labels to one hot encoded vector.\n",
        "\n",
        "      It return all the split data sets '''\n",
        "\n",
        "\n",
        "  # Loading data from online source\n",
        "  (train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()\n",
        "\n",
        "  # Defining labels for data\n",
        "  num_classes = 10\n",
        "  labels=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "  print(\"Number of data points in train data (initially) - \", len(train_x))\n",
        "  print(\"Number of data points in test data (initially) - \", len(test_x))\n",
        "\n",
        "\n",
        "  #performing the train-validation split\n",
        "  train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=40)\n",
        "  \n",
        "\n",
        "  print(\"Shape of each image - 28x28\" )\n",
        "  image_shape=train_x.shape[1]*train_x.shape[2]\n",
        "  print(\"shape of each image (1D) - \",image_shape)\n",
        "  \n",
        "  train_image_count=len(train_x)\n",
        "  val_image_count = len(val_x)\n",
        "  test_image_count=len(test_x)\n",
        "  \n",
        "  # Creating a matrix of image data \n",
        "  # each image is represented as a row by flattening the matrix: converting (60000,28,28) tensor to (60000,784) matrix\n",
        "  X_train=np.zeros((train_image_count,image_shape))\n",
        "  X_val=np.zeros((val_image_count,image_shape))\n",
        "  X_test=np.zeros((test_image_count,image_shape))\n",
        "  \n",
        "  # converting the images into grayscale by normalizing\n",
        "  for i in range(train_image_count):\n",
        "    X_train[i]=(copy.deepcopy(train_x[i].flatten()))/255.0 \n",
        "  for i in range(val_image_count):\n",
        "    X_val[i]=(copy.deepcopy(val_x[i].flatten()))/255.0\n",
        "  for i in range(test_image_count):\n",
        "    X_test[i]=(copy.deepcopy(test_x[i].flatten()))/255.0\n",
        "  \n",
        "\n",
        "\n",
        "  #One hot encoding the label vectors to represent a probability distribution\n",
        "  y_train = np.zeros((train_y.size, 10))\n",
        "  y_train[np.arange(train_y.size), train_y] = 1\n",
        "\n",
        "  y_val = np.zeros((val_y.size, 10))\n",
        "  y_val[np.arange(val_y.size), val_y] = 1\n",
        "\n",
        "  y_test = np.zeros((test_y.size, 10))\n",
        "  y_test[np.arange(test_y.size), test_y] = 1\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  return X_train,X_val,X_test,y_train,y_val,y_test,labels\n",
        "  "
      ],
      "metadata": {
        "id": "uIanRhNjIZiu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting images locally"
      ],
      "metadata": {
        "id": "kM6jDLswWRYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_locally():\n",
        "  xtrain,xval,xtest,ytrain,yval,ytest,labels=prepare_data()\n",
        "  # Creating training dataset\n",
        "  train=np.asarray(list(zip(xtrain,ytrain)))\n",
        "  # plotting a single image from each class\n",
        "  sample_images=[]\n",
        "  wandb_arr=[]\n",
        "  i=1\n",
        "  plt.suptitle(\"Plotting image of each class from Fashion MNIST Dataset\")\n",
        "\n",
        "  while(len(sample_images)!=10):\n",
        "    n=random.randrange(0,len(train))\n",
        "    lab_index=np.asarray(np.nonzero(train[n][1]))[0][0]\n",
        "    \n",
        "    if(lab_index not in sample_images):\n",
        "      plt.subplot(3,5,i)\n",
        "      sample_images.append(lab_index)\n",
        "      plt.title(labels[lab_index])\n",
        "      plt.axis(False)\n",
        "      plt.imshow(train[n][0].reshape((28,28)))\n",
        "      i=i+1"
      ],
      "metadata": {
        "id": "VmT6fEAtWQzV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_locally()"
      ],
      "metadata": {
        "id": "wTFiRD0Zcu2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting images via wandb"
      ],
      "metadata": {
        "id": "nRU6TlqpVSFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#********** Plotting images via wandb************\n",
        "\n",
        "# Enter the entity and project details from wandb.ai\n",
        "wandb.init(entity=\"safikhan\",project=\"trial 1\", name=\"log_images_my\")\n",
        "\n",
        "# Loading dataset\n",
        "xtrain,xval,xtest,ytrain,yval,ytest,labels=prepare_data()\n",
        "\n",
        "# Creating training dataset\n",
        "train=np.asarray(list(zip(xtrain,ytrain)))\n",
        "\n",
        "\n",
        "sample_images=[]\n",
        "wandb_arr=[]\n",
        "i=1\n",
        "plt.suptitle(\"Plotting image of each class from Fashion MNIST Dataset\")\n",
        "while(len(sample_images)!=10):\n",
        "  n=random.randrange(0,len(train))\n",
        "  lab_index=np.asarray(np.nonzero(train[n][1]))[0][0]\n",
        "  if(lab_index not in sample_images):\n",
        "    sample_images.append(lab_index)\n",
        "    wandb_arr.append(wandb.Image(train[n][0].reshape((28,28)),caption=labels[lab_index]))\n",
        "    i=i+1\n",
        "wandb.log({\"images\":wandb_arr})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "-9BOTbyz3rpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334,
          "referenced_widgets": [
            "a958be177adc44e0b87122b1f0b7795c",
            "b0b6e86abb4d452a9c47a92e24b26ceb",
            "4a9f9fa5105a4cbda85b1752ae15fbfc",
            "ee9990dd48a747f4b47c6ac280ee8e91",
            "adad95d14655464b8bcb68caa8f4ebef",
            "741a2c058bc54fec92833b46cd856630",
            "fba0f6ca9f744d0c8d0e37bb9dade147",
            "9ca20cc84df0407db3f2086374452744"
          ]
        },
        "outputId": "386dd660-5c0c-440f-e9b8-ec489e88f3e7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/safikhan/trial%201/runs/ww1olaqe\" target=\"_blank\">log_images_my</a></strong> to <a href=\"https://wandb.ai/safikhan/trial%201\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/safikhan/trial%201/sweeps/4564hvsz\" target=\"_blank\">https://wandb.ai/safikhan/trial%201/sweeps/4564hvsz</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 674... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a958be177adc44e0b87122b1f0b7795c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">log_images_my</strong>: <a href=\"https://wandb.ai/safikhan/trial%201/runs/ww1olaqe\" target=\"_blank\">https://wandb.ai/safikhan/trial%201/runs/ww1olaqe</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220220_022446-ww1olaqe/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \n",
        "\n"
      ],
      "metadata": {
        "id": "IuvK8haH4exM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample Images for each Class :\")\n",
        "class_list=list()\n",
        "wandb.init(entity=\"safikhan\",project=\"trial 1\", name=\"log_images\")\n",
        "\n",
        "for i in range(10):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  for j in range(len(y_train)):\n",
        "    if y_train[j] == i :\n",
        "        wandb.log({\"img\": [wandb.Image(x_train[j], caption=class_type[y_train[j]])]})\n",
        "        class_list.append(class_type[y_train[j]])\n",
        "        break      "
      ],
      "metadata": {
        "id": "QPmxsns1rJjW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "ebb5a859a23746f584fb88c86fc3222f",
            "091e88dfd72d424587b9d10df4fab4d0",
            "eb659b500f9f4019bf822c9de7cf944e",
            "f21fe5e25d5b41a6a2cd2f2007056ef8",
            "a05c9bdeda2a48b39f64b81a76475ce2",
            "db33e8feecd347b9a732e9bac8d67d73",
            "cd81e77088154ba8926a87f69afcbcc6",
            "04c614f49f37416bb4d109b8ddd5d5e1"
          ]
        },
        "outputId": "749fb61b-ec96-48d2-f1c7-124beb03594b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Images for each Class :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:uwql9i2m) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 285... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebb5a859a23746f584fb88c86fc3222f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">log_images</strong>: <a href=\"https://wandb.ai/safikhan/trial%201/runs/uwql9i2m\" target=\"_blank\">https://wandb.ai/safikhan/trial%201/runs/uwql9i2m</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220220_020301-uwql9i2m/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:uwql9i2m). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/safikhan/trial%201/runs/2qwqqtyo\" target=\"_blank\">log_images</a></strong> to <a href=\"https://wandb.ai/safikhan/trial%201\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df2wcdX7/8ef7m8ihpGlIICh0bQUvGwyO5eSOFUFV1QNSLgGUcG2tsJzumuNA4Qr0D6r7IxUNqvghLCHRCsFRoeT4IdSYK1A5LSL8Csch1JyzphzYlybYTlJ7OfVyhFbAySGG9/ePnWzW3nF2crvxendeD2mUmc98xjt5ZfLenf3s+mPujoiIxMP/q/UJiIjIzFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiZGyRd/MfmxmvzazgWn2m5k9YmZDZva+mX29aN8mM/swWDZV88RrTbmUUiallEk45VJD7n7KBfgT4OvAwDT7rwNeBgy4Avh50L4YGAn+XBSsLyr3ePWyKBdlokyUSz0uZV/pu/vPgKOn6HID8Izn7QHOMbMLgLXAa+5+1N0/AV4D1pV7vHqhXEopk1LKJJxyqZ25VfgZCWC0aHssaJuuvYSZbQY2A8yfP/+ySy65pAqndeZ1dHQwNDREOp0u+VrzwoULWbp06eZ0Ov3PAAsWLCCRSHz06aef4u6Y2Z+7+xIaLBdlUqqSTNLp9Ob+/v7fAI/QQJmArpVq6+/v/02QyalFuR0ALmT627B/B/64aPsNIA38EPi7ovatwA/LPdZll13m9eLgwYO+YsWK0H3XX3+9v/3224Xtq6++2vfu3esPPfSQ33fffQ5kvQFzUSalKsnE3R3INlom7rpWqu1EJuWWanx6Jwe0FG03B23TtcdCIpFgdPTkjc7Y2BiJRKKknRjlokxKKZNwyuXMqUbR3wn8ZTDafgXwf+7+K+AV4JtmtsjMFgHfDNpiYcOGDTzzzDO4O3v27GHhwoVccMEFrF27lldffRVgTtxyUSalymXyySefAMwhRpmArpUzqtytALAD+BVwnPz7Z7cAPwB+EOw34DFgGPgASBcd+31gKFhujnLrUS+3YZlMxpcuXepz5871RCLh27Zt88cff9wff/xxd3f/6quv/Pbbb/dkMukdHR2+d+/ewrHbt293YLzRclEmpSrN5KKLLjqRS8Nk4q5r5Uwg4ts7lu87e6TTac9ms7U+jTPOzPrdPR21fxxyUSbhTicXZRIuDrlEzUTfyBURiREVfRGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYiVT0zWydme03syEz2xKy/x/M7L1gOWBm/1u078uifTurefK1tGvXLtra2kilUnR3d5fsv+uuu1i1ahWrVq3i4osv5pxzzinsmzNnDkC7Mmn8TEC5hFEmNVRuai3y83MOA0mgCfgF0H6K/n8N/Lho+7MoU3idWOphWrOJiQlPJpM+PDzsx44d887OTh8cHJy2/yOPPOI333xzYXv+/PmRpzbzOslFmYSb6VyUSf3mUqmomUR5pX85MOTuI+7+BdAD3HCK/jeRn1e3YfX19ZFKpUgmkzQ1NZHJZOjt7Z22/44dO7jppptm8AxnnjIJp1xKKZPailL0E8Bo0fZY0FbCzJYBrcDuouazzCxrZnvM7FvTHLc56JM9cuRIxFOvnVwuR0tLS2G7ubmZXC4X2vfw4cMcPHiQq6++utA2Pj4OcOmpMoH6ykWZhJuJXJRJuHrLZaZUeyA3Azzv7l8WtS3z/GS93wb+0cwumnqQuz/h7ml3Ty9ZsqTKp1RbPT09dHV1nXgfEshfyMA+TpEJNG4uyiTc75qLMonftVKJKEU/B7QUbTcHbWEyTHlrx91zwZ8jwE+Br532Wc4yiUSC0dGTNz9jY2MkEqE3P/T09JTcmp7oq0wmHw+NlQkolzDKpMbKvekPzAVGyL9tc2Igd0VIv0uAQ4AVtS0C5gXr5wEfcopBYK+TAZfjx497a2urj4yMFAaiBgYGSvrt27fPly1b5l999VWh7ejRoz4+Pu5ANmomXge5KJNwM52LMqnfXCpFtQZy3X0CuBN4hfwt1U/cfdDM7jWzDUVdM0BP8OAnXApkzewXwJtAt7v/MtrT0ew1d+5cHn30UdauXcull17Kxo0bWbFiBffccw87d578BFlPTw+ZTAYzK7Tt27ePdDoN0I4yARo3E1AuYZRJbdnkGl176XTas9lsrU/jjDOzfs+PdUQSh1yUSbjTyUWZhItDLlEz0TdyRURiREVfRCRGVPRFRGJERV9EJEZU9EVEYkRFX0QkRlT0RURiREVfRCRGVPRFRGJERV9EJEZU9EVEYkRFX0QkRlT0RURiREVfRCRGVPRFRGJERV9EJEYiFX0zW2dm+81syMy2hOz/npkdMbP3guXWon2bzOzDYNlUzZOvpV27dtHW1kYqlaK7u7tk/1NPPcWSJUtYtWoVq1atYtu2bYV9Tz/9NECHMmn8TEC5hFEmNVRuPkVgDjAMJDk5R277lD7fAx4NOXYx+fl1F5OfL3cEWHSqx6uHuSwnJiY8mUz68PBwYY7PwcHBSX2efPJJv+OOO0qO/fjjj721tdWB/4yaiddBLsok3EznokzqN5dKUa05coHLgSF3H3H3L4Ae4IaIzylrgdfc/ai7fwK8BqyLeOys1dfXRyqVIplM0tTURCaTobe3N9Kxr7zyCtdccw3Al8okr1EzAeUSRpnUVpSinwBGi7bHgrap/sLM3jez582s5XSONbPNZpY1s+yRI0cinnrt5HI5WlpaCtvNzc3kcrmSfi+88AKdnZ10dXUxOjoaeizT51lXuSiTcDORizJpjGtlplRrIPffgAvdvZP8M+/Tp3Owuz/h7ml3Ty9ZsqRKp1Rb69ev59ChQ7z//vtcc801bNp0+m89NlouyiRcpbkok3CNmEs1RCn6OaD4qbU5aCtw94/d/ViwuQ24LOqx9SiRSBReeQCMjY2RSEx+sXHuuecyb948AG699Vb6+/tDj0WZNGwmoFzCKJMaK/emPzCX/GBJKycHcldM6XNB0fqfAXv85EDuQfIDLouC9cWnerx6GHA5fvy4t7a2+sjISGEgamBgYFKfjz76qLD+4osv+urVq909PxB14YUXFg9Elc3E6yAXZRJupnNRJvWbS6WIOJBbtkP+Z3EdcID8p3juDtruBTYE6w8Cg8ETwpvAJUXHfh8YCpabyz1WvfzjvPTSS758+XJPJpN+//33u7v71q1bvbe3193dt2zZ4u3t7d7Z2elXXnml79u3r3Ds9u3bHRiPmonXSS7KJNxM5qJM6juXSkQt+pbvO3uk02nPZrO1Po0zzsz63T0dtX8cclEm4U4nF2USLg65RM1E38gVEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiZFIRd/M1pnZfjMbMrMtIfv/xsx+GUyM/oaZLSva96WZvRcsO6t58rW0a9cu2traSKVSdHd3l+x/+OGHaW9vp7OzkzVr1nD48OHCvjlz5gC0K5PGzwSUSxhlUkPlZlkB5pCfMSvJyekS26f0uQo4O1j/K+C5on2fRZnN5cRSDzPcTExMeDKZ9OHh4cJ0b4ODg5P67N692z///HN3d//Rj37kGzduLOybP39+5FluvE5yUSbhZjoXZVK/uVQqaiZRXulfDgy5+4i7fwH0ADdMeeJ4091/G2zuIT9ZccPq6+sjlUqRTCZpamoik8nQ29s7qc9VV13F2WefDcAVV1zB2NhYLU51xiiTcMqllDKprShFPwEUTz8/FrRN5xbg5aLts8wsa2Z7zOxbYQeY2eagT/bIkSMRTqm2crkcLS0the3m5mZyudy0/bdv3861115b2B4fHwe49FSZQH3lokzCzUQuyiRcveUyU+ZW84eZ2XeANPCNouZl7p4zsySw28w+cPfh4uPc/QngCcjPZVnNc6q1Z599lmw2y1tvvVVoO3z4MM3NzfuAbzNNJtC4uSiTcL9rLsokftdKJaIU/RzQUrTdHLRNYmZ/CtwNfMPdj51od/dc8OeImf0U+Br5MYK6lUgkGB09efMzNjZGIlF68/P666/zwAMP8NZbbzFv3rxJx4MyafRMQLmEUSY1Vu5Nf/JPDCNAKycHcldM6XMi9OVT2hcB84L184APmTIIPHWphwGX48ePe2trq4+MjBQGogYGBib1effddz2ZTPqBAwcmtR89etTHx8cdyEbNxOsgF2USbqZzUSb1m0uliDiQG2nkG7gOOBAU9ruDtnuBDcH668D/AO8Fy86g/Y+AD4Inig+AW8o9Vr3847z00ku+fPlyTyaTfv/997u7+9atW723t9fd3desWePnn3++r1y50leuXOnr1693d/d33nnHOzo6HPht1Ey8TnJRJuFmMhdlUt+5VCJq0bd839kjnU57Nput9WmccWbW7+7pqP3jkIsyCXc6uSiTcHHIJWom+kauiEiMqOiLiMSIir6ISIyo6IuIxIiKvohIjKjoi4jEiIq+iEiMqOiLiMSIir6ISIyo6IuIxIiKvohIjKjoi4jEiIq+iEiMqOiLiMSIir6ISIyo6IuIxEikom9m68xsv5kNmdmWkP3zzOy5YP/PzezCon1/G7TvN7O11Tv12tq1axdtbW2kUim6u7tL9h87dowbb7yRVCrF6tWrOXToUGHfgw8+CNDRaJlAZbkAS3WtxONaUSY1VG5qLWAO+WkSk5ycI7d9Sp/bgX8K1jPAc8F6e9B/Hvk5doeBOad6vHqY1mxiYsKTyaQPDw8X5vgcHByc1Oexxx7z2267zd3dd+zY4Rs3bnR398HBQe/s7HSgP2omHpNcyE+Bp2ulgmtFmdRvLpUi4nSJUV7pXw4MufuIu38B9AA3TOlzA/B0sP48sMbMLGjvcfdj7n4QGAp+Xl3r6+sjlUqRTCZpamoik8nQ29s7qU9vby+bNm0CoKurizfeeAN3p7e3l0wmA+CNlAlUngtwVNdK418ryqS2ys6Ra2ZdwDp3vzXY/i6w2t3vLOozEPQZC7aHgdXA3wN73P3ZoH078LK7Pz/lMTYDm4PNDmCg8r/aGbUI+APgcLC9GPh94L+L+qwgP5n88WC7A/gv4A+Bz4Al7r5gukwglrkscPffA10rnMa1okwa5v9PpdrcfUHZXuVuBYAuYFvR9neBR6f0GQCai7aHgfOAR4HvFLVvB7rKPF6kW5RaLtXI5MTfM0omMcplRNdKZdeKMqnfXKqQa9Xe3skBLUXbzUFbaB8zmwssBD6OeGw9UibhKs2lqcyx9UjXSillUkNRiv5eYLmZtZpZE/mB2p1T+uwENgXrXcBuzz/17AQywad7WoHlQF91Tr2mKs4EsAbLBCrPZbGulVhcK8qkliLeNlxH/v21YeDuoO1eYEOwfhbwL+QHVfqAZNGxdwfH7QeujfBYm2t9mzRDmfw6aiYxyuVfda1Udq0ok/rOpcJMI/0dyw7kiohI49A3ckVEYkRFX0QkRmZV0S/36x4agZn92Mx+HXy3IUp/ZVLav+EzAeUSRpmUOt1Maj74UDQIUfbXPTTCAvwJ8HVgQJkoE+WiTGYyE/don9OfKVF+3UPdc/efAUcjdlcmpWKRCSiXMMqk1GlmMquKfgIYLdoeC9riTJmUUibhlEspZRJiNhV9ERE5w2ZT0dfXq0spk1LKJJxyKaVMQsymoh/lq9lxo0xKKZNwyqWUMgkxa4q+u08AdwKvAPuAn7j7YG3PqvrMbAfwH0CbmY2Z2S3T9VUmpeKSCSiXMMqk1OlkAhF+n76IiDSOsq/0y33w3/IeCb788L6Zfb1o3yYz+zBYNoUdX6+USyllUkqZhFMuNVTpB//J/7a8lwEDrgB+HrQvBkaCPxcF64tq/UWGmfpCRBxzUSbKRLnM/qXsK30v/8H/G4BnPG8PcI6ZXQCsBV5z96Pu/gnwGrCu3OPVC+VSSpmUUibhlEvtzK3Cz5juCxCRvxhhRXNZzp8//7JLLrmkCqd15nV0dDA0NEQ6nS4ZGFm4cCFLly7dnE6n/xlgwYIFJBKJjz799FPcHTP7c3dfQoPlokxKVZJJOp3e3N/f/xvgERooE9C1Um39/f2/CTI5tSi3A8CFTH8b9u/AHxdtvwGkgR8Cf1fUvhX4YbnHuuyyy7xeHDx40FesWBG67/rrr/e33367sH311Vf73r17/aGHHvL77rvPOTnHZ0PlokxKVZKJuzuQbbRM3HWtVBtVnCO3nOm+ABHrL0YkEglGR0/e6IyNjZFIJEraiVEuyqSUMgmnXM6cahT9ncBfBqPtVwD/5+6/Iv/Z2G+a2SIzWwR8M2iLhQ0bNvDMM8/g7uzZs4eFCxdywQUXsHbtWl599VWAOXHLRZmUKpfJJ598AvnfFhmbTEDXyhlV7lYA2AH8CjhO/v2zW4AfAD8I9hvwGPlfYfoBkC469vvk57gcAm6OcutRL7dhmUzGly5d6nPnzvVEIuHbtm3zxx9/3B9//HF3d//qq6/89ttv92Qy6R0dHb53797Csdu3b3dgvNFyUSalKs3koosuOpFLw2TirmvlTCDi2zuz7stZ6XTas9lsrU/jjDOzfndPR+0fh1yUSbjTyUWZhItDLlEzmTW/hkFERM48FX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGIlU9M1snZntN7MhM9sSsv8fzOy9YDlgZv9btO/Lon07q3nytbRr1y7a2tpIpVJ0d3eX7L/rrrtYtWoVq1at4uKLL+acc84p7JszZw5AuzJp/ExAuYRRJjVUbmot8vNzDgNJoAn4BdB+iv5/Dfy4aPuzKFN4nVjqYVqziYkJTyaTPjw87MeOHfPOzk4fHByctv8jjzziN998c2F7/vz5kac28zrJRZmEm+lclEn95lKpqJlEeaV/OTDk7iPu/gXQA9xwiv43kZ9Xt2H19fWRSqVIJpM0NTWRyWTo7e2dtv+OHTu46aabZvAMZ54yCadcSimT2opS9BPAaNH2WNBWwsyWAa3A7qLms8wsa2Z7zOxb0xy3OeiTPXLkSMRTr51cLkdLS0thu7m5mVwuF9r38OHDHDx4kKuvvrrQNj4+DnDpqTKB+spFmYSbiVyUSbh6y2WmVHsgNwM87+5fFrUt8/xkvd8G/tHMLpp6kLs/4e5pd08vWbKkyqdUWz09PXR1dZ14HxLIX8jAPk6RCTRuLsok3O+aizKJ37VSiShFPwe0FG03B21hMkx5a8fdc8GfI8BPga+d9lnOMolEgtHRkzc/Y2NjJBKhNz/09PSU3Jqe6KtMJh8PjZUJKJcwyqTGyr3pD8wFRsi/bXNiIHdFSL9LgEOAFbUtAuYF6+cBH3KKQWCvkwGX48ePe2trq4+MjBQGogYGBkr67du3z5ctW+ZfffVVoe3o0aM+Pj7uQDZqJl4HuSiTcDOdizKp31wqRbUGct19ArgTeIX8LdVP3H3QzO41sw1FXTNAT/DgJ1wKZM3sF8CbQLe7/zLa09HsNXfuXB599FHWrl3LpZdeysaNG1mxYgX33HMPO3ee/ARZT08PmUwGMyu07du3j3Q6DdCOMgEaNxNQLmGUSW3Z5Bpde+l02rPZbK1P44wzs37Pj3VEEodclEm408lFmYSLQy5RM9E3ckVEYkRFX0QkRlT0RURiREVfRCRGVPRFRGJERV9EJEZU9EVEYkRFX0QkRlT0RURiREVfRCRGVPRFRGJERV9EJEZU9EVEYkRFX0QkRlT0RURiJFLRN7N1ZrbfzIbMbEvI/u+Z2REzey9Ybi3at8nMPgyWTdU8+VratWsXbW1tpFIpuru7S/Y/9dRTLFmyhFWrVrFq1Sq2bdtW2Pf0008DdCiTxs8ElEsYZVJD5abWAuYAw0CSk9Mltk/p8z3g0ZBjF5OfanEx+akTR4BFp3q8epjWbGJiwpPJpA8PDxemexscHJzU58knn/Q77rij5NiPP/7YW1tbHfjPqJl4HeSiTMLNdC7KpH5zqRTVmi4RuBwYcvcRd/8C6AFuiPicshZ4zd2PuvsnwGvAuojHzlp9fX2kUimSySRNTU1kMhl6e3sjHfvKK69wzTXXAHypTPIaNRNQLmGUSW1FKfoJYLRoeyxom+ovzOx9M3vezFpO51gz22xmWTPLHjlyJOKp104ul6OlpaWw3dzcTC6XK+n3wgsv0NnZSVdXF6Ojo6HHMn2edZWLMgk3E7kok8a4VmZKtQZy/w240N07yT/zPn06B7v7E+6edvf0kiVLqnRKtbV+/XoOHTrE+++/zzXXXMOmTaf/1mOj5aJMwlWaizIJ14i5VEOUop8Dip9am4O2Anf/2N2PBZvbgMuiHluPEolE4ZUHwNjYGInE5Bcb5557LvPmzQPg1ltvpb+/P/RYlEnDZgLKJYwyqbFyb/oDc8kPlrRyciB3xZQ+FxSt/xmwx08O5B4kP+CyKFhffKrHq4cBl+PHj3tra6uPjIwUBqIGBgYm9fnoo48K6y+++KKvXr3a3fMDURdeeGHxQFTZTLwOclEm4WY6F2VSv7lUiogDuWU75H8W1wEHyH+K5+6g7V5gQ7D+IDAYPCG8CVxSdOz3gaFgubncY9XLP85LL73ky5cv92Qy6ffff7+7u2/dutV7e3vd3X3Lli3e3t7unZ2dfuWVV/q+ffsKx27fvt2B8aiZeJ3kokzCzWQuyqS+c6lE1KJv+b6zRzqd9mw2W+vTOOPMrN/d01H7xyEXZRLudHJRJuHikEvUTPSNXBGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJERV9EZEYUdEXEYmRSEXfzNaZ2X4zGzKzLSH7/8bMfmlm75vZG2a2rGjfl2b2XrDsrObJ19KuXbtoa2sjlUrR3d1dsv/hhx+mvb2dzs5O1qxZw+HDhwv75syZA9CuTBo/E1AuYZRJDZWbWguYQ36axCQn58htn9LnKuDsYP2vgOeK9n0WZQqvE0s9TGs2MTHhyWTSh4eHC3N8Dg4OTuqze/du//zzz93d/Uc/+pFv3LixsG/+/PmRpzbzOslFmYSb6VyUSf3mUqmomUR5pX85MOTuI+7+BdAD3DDlieNNd/9tsLmH/Az1Dauvr49UKkUymaSpqYlMJkNvb++kPldddRVnn302AFdccQVjY2O1ONUZo0zCKZdSyqS2ohT9BDBatD0WtE3nFuDlou2zzCxrZnvM7FthB5jZ5qBP9siRIxFOqbZyuRwtLS2F7ebmZnK53LT9t2/fzrXXXlvYHh8fB7j0VJlAfeWiTMLNRC7KJFy95TJT5lbzh5nZd4A08I2i5mXunjOzJLDbzD5w9+Hi49z9CeAJyE9gXM1zqrVnn32WbDbLW2+9VWg7fPgwzc3N+4BvM00m0Li5KJNwv2suyiR+10olohT9HNBStN0ctE1iZn8K3A18w92PnWh391zw54iZ/RT4GvkxgrqVSCQYHT158zM2NkYiUXrz8/rrr/PAAw/w1ltvMW/evEnHgzJp9ExAuYRRJjVW7k1/8k8MI0ArJwdyV0zpcyL05VPaFwHzgvXzgA+ZMgg8damHAZfjx497a2urj4yMFAaiBgYGJvV59913PZlM+oEDBya1Hz161MfHxx3IRs3E6yAXZRJupnNRJvWbS6WIOJAbaeQbuA44EBT2u4O2e4ENwfrrwP8A7wXLzqD9j4APgieKD4Bbyj1WvfzjvPTSS758+XJPJpN+//33u7v71q1bvbe3193d16xZ4+eff76vXLnSV65c6evXr3d393feecc7Ojoc+G3UTLxOclEm4WYyF2VS37lUImrRt3zf2SOdTns2m631aZxxZtbv7umo/eOQizIJdzq5KJNwccglaib6Rq6ISIyo6IuIxIiKvohIjKjoi4jEiIq+iEiMqOiLiMSIir6ISIyo6IuIxIiKvohIjKjoi4jEiIq+iEiMqOiLiMSIir6ISIyo6IuIxIiKvohIjKjoi4jESKSib2brzGy/mQ2Z2ZaQ/fPM7Llg/8/N7MKifX8btO83s7XVO/Xa2rVrF21tbaRSKbq7u0v2Hzt2jBtvvJFUKsXq1as5dOhQYd+DDz4I0NFomUBluQBLda3E41pRJjVUbmotYA75aRKTnJwjt31Kn9uBfwrWM8BzwXp70H8e+Tl2h4E5p3q8epjWbGJiwpPJpA8PDxfm+BwcHJzU57HHHvPbbrvN3d137NjhGzdudHf3wcFB7+zsdKA/aiYek1zIT4Gna6WCa0WZ1G8ulSLidIlRXulfDgy5+4i7fwH0ADdM6XMD8HSw/jywxswsaO9x92PufhAYCn5eXevr6yOVSpFMJmlqaiKTydDb2zupT29vL5s2bQKgq6uLN954A3ent7eXTCYD4I2UCVSeC3BU10rjXyvKpLbKzpFrZl3AOne/Ndj+LrDa3e8s6jMQ9BkLtoeB1cDfA3vc/dmgfTvwsrs/P+UxNgObg80OYKDyv9oZtQj4A+BwsL0Y+H3gv4v6rCA/mfzxYLsD+C/gD4HPgCXuvmC6TCCWuSxw998DXSucxrWiTBrm/0+l2tx9Qdle5W4FgC5gW9H2d4FHp/QZAJqLtoeB84BHge8UtW8Huso8XqRblFou1cjkxN8zSiYxymVE10pl14oyqd9cqpBr1d7eyQEtRdvNQVtoHzObCywEPo54bD1SJuEqzaWpzLH1SNdKKWVSQ1GK/l5guZm1mlkT+YHanVP67AQ2BetdwG7PP/XsBDLBp3tageVAX3VOvaYqzgSwBssEKs9lsa6VWFwryqSWIt42XEf+/bVh4O6g7V5gQ7B+FvAv5AdV+oBk0bF3B8ftB66N8Fiba32bNEOZ/DpqJjHK5V91rVR2rSiT+s6lwi3xb6kAAAEjSURBVEwj/R3LDuSKiEjj0DdyRURiREVfRCRGZlXRL/frHhqBmf3YzH4dfLchSn9lUtq/4TMB5RJGmZQ63UxqPvhQNAhR9tc9NMIC/AnwdWBAmSgT5aJMZjIT92if058pUX7dQ91z958BRyN2VyalYpEJKJcwyqTUaWYyq4p+Ahgt2h4L2uJMmZRSJuGUSyllEmI2FX0RETnDZlPR19erSymTUsoknHIppUxCzKaiH+Wr2XGjTEopk3DKpZQyCTFrir67TwB3Aq8A+4CfuPtgbc+q+sxsB/AfQJuZjZnZLdP1VSal4pIJKJcwyqTU6WQCEX6fvoiINI5Z80pfRETOPBV9EZEYUdEXEYkRFX0RkRhR0RcRiREVfRGRGFHRFxGJkf8PO4s6HkMR4NAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()"
      ],
      "metadata": {
        "id": "ntsvInuZrJhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ddec3d-54be-4f30-b54c-cb7ab452d4b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Defining Various Utility functions</h3>"
      ],
      "metadata": {
        "id": "7OI2Z4lEK1Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Initialization Functions</h4>"
      ],
      "metadata": {
        "id": "cBekYpc66E6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Xavier(layer_sizes):\n",
        "  params = {}\n",
        "  for i in range(1,len(layer_sizes)):\n",
        "      norm_xav=np.sqrt(6)/np.sqrt(layer_sizes[i]+layer_sizes[i-1])\n",
        "      params[\"w\"+str(i)]=np.random.randn(layer_sizes[i],layer_sizes[i-1])*norm_xav\n",
        "      params[\"b\"+str(i)]=np.zeros((layer_sizes[i],1))\n",
        "  \n",
        "  return params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nUVnJQfuUHqb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Random(layer_sizes):\n",
        "  params = {}\n",
        "  for i in range(1,len(layer_sizes)):\n",
        "      params[\"w\"+str(i)]=0.01*np.random.randn(layer_sizes[i],layer_sizes[i-1])\n",
        "      params[\"b\"+str(i)]=0.01*np.random.randn(layer_sizes[i],1)\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "1sRkD2gu46dL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Activation Functions </h4>"
      ],
      "metadata": {
        "id": "I7VIhXa56Zi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(pre_act):\n",
        "  try:\n",
        "    return (1.0/(1.0+np.exp(-pre_act)))\n",
        "  except:\n",
        "    print(\"error in sigmoid\")"
      ],
      "metadata": {
        "id": "ZM7quY3l6PZZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(pre_act):\n",
        "  return (np.tanh(pre_act))\n"
      ],
      "metadata": {
        "id": "dAvWjzlz6RjK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(pre_act):\n",
        "  return (np.maximum(0,pre_act))"
      ],
      "metadata": {
        "id": "D8ZpvzQu6TZt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  try:\n",
        "    return(np.exp(x)/np.sum(np.exp(x)))\n",
        "  except:\n",
        "    print(\"error in softmax\")"
      ],
      "metadata": {
        "id": "8Vq4OnO96WRI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hMEinOgW6XqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Derivatives of Activation Functions </h4>"
      ],
      "metadata": {
        "id": "PZcX9QpD6keQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))"
      ],
      "metadata": {
        "id": "6SgvWXU76reg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh_derivative(x):\n",
        "  return 1.0 -tanh(x)**2\n"
      ],
      "metadata": {
        "id": "EFMQdq_b6rcV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(x):\n",
        "  return 1. * (x>0)"
      ],
      "metadata": {
        "id": "bUXYoFkF6rZ-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_derivative(x):\n",
        "  return softmax(x) * (1-softmax(x))"
      ],
      "metadata": {
        "id": "Up46ku0y6rRm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative(A, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    return sigmoid_derivative(A)\n",
        "  elif activation == \"tanh\":\n",
        "    return tanh_derivative(A)\n",
        "  elif activation == \"relu\":\n",
        "    return relu_derivative(A)\n"
      ],
      "metadata": {
        "id": "rHgm-ZbS6OLW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Loss Functions</h4>"
      ],
      "metadata": {
        "id": "56KNthDl67G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y, y_hat):\n",
        "  error = np.sum(((y - y_hat)**2) / (2 * len(y)))\n",
        "  return error"
      ],
      "metadata": {
        "id": "PRnQH-867Ayy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossEntropy(y, y_hat):\n",
        "  error = - np.sum( np.multiply(y , np.log(y_hat)))/len(y)\n",
        "  return error"
      ],
      "metadata": {
        "id": "YP4QzvRF7Xw-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculating loss \n",
        "def loss_calc(loss_name, y, y_hat, lambd, layer_sizes, parameters):\n",
        "  error=0\n",
        "  if(loss_name == \"sse\"):\n",
        "    error=MSE(y, y_hat)\n",
        "  elif(loss_name == \"cross_entropy\"):\n",
        "    error=CrossEntropy(y, y_hat)\n",
        "    #error = -np.sum(np.sum(y_t*np.log(y_hat)))\n",
        "\n",
        "  #For L2 Regularization\n",
        "  regularized_error = 0.0\n",
        "  for i in range(len(layer_sizes)-1, 0, -1):\n",
        "    regularized_error += (np.sum(parameters[\"w\"+str(i)]))**2\n",
        "  regularized_error = error + ((lambd/(2*len(y)))*(regularized_error))\n",
        "\n",
        "\n",
        "  return regularized_error"
      ],
      "metadata": {
        "id": "MyC7PPN16-Sn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Accuracy <h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "j1tkJKxj0QzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(res,y_t):\n",
        "    # res is the vector that comes \n",
        "    acc=0.0\n",
        "    \n",
        "    for x in range(len(res)):\n",
        "      if(res[x].argmax()==y_t[x].argmax()):\n",
        "        acc+=1\n",
        "    acc=acc/len(y_t)\n",
        "    return(acc*100)"
      ],
      "metadata": {
        "id": "H-FBfniHt5I_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bghjsd1F6-QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Initialization of Neural Network</h2>"
      ],
      "metadata": {
        "id": "RvRGfWa078JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_init(layer_sizes, init_type = \"random\"):\n",
        "  # Layer Sizes denotes the number of neurons per layer\n",
        "  # 784 is for the input layer. \n",
        "  # 32 is for the hidden layers. \n",
        "  # 10 is for the output layers\n",
        "\n",
        "  # initializing parameters for the neural network, \n",
        "  params={}\n",
        "  if(init_type==\"xavier\"):\n",
        "    params = Xavier(layer_sizes)\n",
        "\n",
        "  elif(init_type==\"random\"):\n",
        "    params = Random(layer_sizes)\n",
        "\n",
        "  else:\n",
        "    print(\"Enter a valid weight initilization type\")\n",
        "\n",
        "  return params\n"
      ],
      "metadata": {
        "id": "2ZICexToBSP0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Forward Propagation</h2>"
      ],
      "metadata": {
        "id": "Noyewh3N8QGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X,y,params,active,layer_sizes):\n",
        "  \n",
        "  # Extracting only the image data not the label for the image data\n",
        "  out=copy.deepcopy(X)\n",
        "  out=out.reshape(-1,1)\n",
        "  \n",
        "  #These are stored just to make it easy to keep track of the indices along with layers.\n",
        "  h=[out] # To save the activations for each neuron in a layer\n",
        "  a=[out] # To save the preactivation for each neuron in a layer\n",
        "\n",
        "  if(active==\"sigmoid\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=sigmoid(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"tanh\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=tanh(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"relu\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=relu(out)\n",
        "      h.append(post_a)       \n",
        "  else:\n",
        "    print(\"Enter a valid activation function\") \n",
        "\n",
        "  # Final step for forward propagation, using softmax.\n",
        "  weights=params[\"w\"+str(len(layer_sizes)-1)]\n",
        "  biases=params[\"b\"+str(len(layer_sizes)-1)]\n",
        "  \n",
        "  out=np.dot(weights,h[len(layer_sizes)-2])+biases\n",
        "  a.append(out)\n",
        "  y_hat=softmax(out)\n",
        "  h.append(y_hat)\n",
        "  \n",
        "  \n",
        "  #in h we  are storing values for layers right from input till output\n",
        "  #h0 is input\n",
        "  #in a we are storing values for layers right from input till output\n",
        "  #a0 is input\n",
        "\n",
        "  return h,a,y_hat"
      ],
      "metadata": {
        "id": "8VaQdyqbrqsO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Back Propagation</h2>"
      ],
      "metadata": {
        "id": "wmjefdiK8cxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(y, y_hat, h, a, params, loss_type, layer_sizes, activation):\n",
        "  \n",
        "  #here both y_hat and y are assumed to be column vectors\n",
        "\n",
        "\n",
        "\n",
        "  grad = {}\n",
        "\n",
        "  if loss_type == \"squared_error\":\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = (y_hat - y)\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = (y_hat - y) * softmax_derivative(a[len(layer_sizes)-1])\n",
        "\n",
        "  elif loss_type == 'cross_entropy':\n",
        "    #Here actually it should be one hot vector. But y does the same job\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = -(y-y_hat)\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = -(y/y_hat)\n",
        "\n",
        "  for i in range(len(layer_sizes)-1, 0, -1 ):\n",
        "    #print(i)\n",
        "    #Not considering L2 Regularization here. Instead will cumulate in the update section\n",
        "    grad[\"dw\"+str(i)] = np.dot(grad[\"da\"+str(i)], np.transpose(h[i-1]))\n",
        "    grad[\"db\"+str(i)] = grad[\"da\"+str(i)]\n",
        "\n",
        "    if i > 1:\n",
        "      grad[\"dh\"+str(i-1)] = np.dot(np.transpose(params[\"w\"+str(i)]), grad[\"da\"+str(i)])\n",
        "      grad[\"da\"+str(i-1)] = np.multiply(grad[\"dh\" + str(i-1)], derivative(a[i-1],activation))\n",
        " \n",
        "  return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "kAQEfBMIFfW0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate gradients, batchwise"
      ],
      "metadata": {
        "id": "cYFgtV7ZIOCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function):\n",
        "  grads={}\n",
        "  grads.clear()\n",
        "  #iterate over all the points in the current batch\n",
        "  for j in range(len(X)):\n",
        "    y = np.reshape(Y[j], (-1,1))\n",
        "    #Feed forward the data point\n",
        "    h,a,y_hat = forward_prop(X[j], y, parameters, activation, layers)\n",
        "    #backpropagate the error.\n",
        "    new_grads = back_prop(y,y_hat, h,a, parameters, loss_function, layers, activation)\n",
        "    #keep collecting the gradients for all the data (since vanilla GD)\n",
        "    if j == 0:\n",
        "      grads = copy.deepcopy(new_grads)\n",
        "    else:\n",
        "      for k in range(len(layers)-1,0,-1):\n",
        "        grads[\"dw\"+str(k)] += new_grads[\"dw\"+str(k)]\n",
        "        grads[\"db\"+str(k)] += new_grads[\"db\"+str(k)]\n",
        "  \n",
        "  return grads"
      ],
      "metadata": {
        "id": "CJXUB3SdINv8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Different Optimization Functions</h2>"
      ],
      "metadata": {
        "id": "JhnoYTOQ8g9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Mini Batch Gradient Descent </h3>"
      ],
      "metadata": {
        "id": "KKHa2-Fh8lye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gd(X_train, y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters):\n",
        "  #parameters = nn_init(layers, 'random')\n",
        "  \n",
        "  grads={}\n",
        "  \n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads = grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "    \n",
        "      #Updating the parameters once every one batch\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - (eta * grads[\"dw\"+str(j)])\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - (eta * grads[\"db\"+str(j)])\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "H7a01Svldakv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Momentum Based Gradient Descent </h3>"
      ],
      "metadata": {
        "id": "yPiDf0vS8vsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gd(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "  #parameters = nn_init(layers, 'random')\n",
        "  \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  gamma = 0.9 #Not treating this as a hyperparameter\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "      #Storing the update history for each parameter.\n",
        "      if i == 0 :\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = eta*grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = eta*grads[\"db\"+str(j)]\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (gamma*update_history[\"w\"+str(j)]) + (eta*grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (gamma*update_history[\"b\"+str(j)]) + (eta*grads[\"db\"+str(j)])\n",
        "\n",
        "    \n",
        "      #Updating the parameters once every one batch with the update_history\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "NHgO8wzQ8-qD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Nesterov Accelerated Gradient Descent</h3>"
      ],
      "metadata": {
        "id": "zXL670qB879E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gd(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        " \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  param_lookahead = {}\n",
        "  gamma = 0.9 #not treating this as a hyperparameter.\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      #If it is the first batch, we still dont have the previous history.\n",
        "      #So, lookahead will be same as the current parameters\n",
        "      if i==0:\n",
        "        param_lookahead = copy.deepcopy(parameters)\n",
        "      \n",
        "      #If its not the first batch then we calculate lookahead according to\n",
        "      #the formula.\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          param_lookahead['w'+str(j)] = parameters['w'+str(j)] + (gamma*update_history[\"w\"+str(j)])\n",
        "                                                                  \n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,param_lookahead,activation,layers,loss_function)\n",
        "      \n",
        "      #Storing the update history for each parameter.\n",
        "\n",
        "      #If its the first batch, we dont have any update history yet. So, it will\n",
        "      #be same as the eta*gradients\n",
        "      if i == 0 :\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = eta*grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = eta*grads[\"db\"+str(j)]\n",
        "      \n",
        "      #If its not the first batch, we cumulate the update history as per the \n",
        "      #formula.\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (gamma*update_history[\"w\"+str(j)]) + (eta*grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (gamma*update_history[\"b\"+str(j)]) + (eta*grads[\"db\"+str(j)])\n",
        "\n",
        "    \n",
        "      #Updating the parameters once every one batch with the update_history\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "WEglGxnFE3Fg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> RMSprop</h3>"
      ],
      "metadata": {
        "id": "xNB0DGgD9Ck7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for rmsprop***************\n",
        "def rmsprop(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  \n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta = 0.9 \n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "   \n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "        \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "        v[\"w\"+str(iq)]=beta*v[\"w\"+str(iq)]+(1-beta)*grads[\"dw\"+str(iq)]**2\n",
        "        v[\"b\"+str(iq)]=beta*v[\"b\"+str(iq)]+(1-beta)*grads[\"db\"+str(iq)]**2\n",
        "          \n",
        "        update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(v[\"w\"+str(iq)]+epsilon)),grads[\"dw\"+str(iq)])\n",
        "        update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(v[\"b\"+str(iq)]+epsilon)),grads[\"db\"+str(iq)])\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22wMGB7d9Cwc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Adam </h3>"
      ],
      "metadata": {
        "id": "31e2lLRk9YSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for adam***************\n",
        "def adam(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "  m={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing m \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    m[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    m[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta1 = 0.9 \n",
        "  beta2=0.999\n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "      grads.clear()\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "          m[\"w\"+str(iq)]=beta1*m[\"w\"+str(iq)]+(1-beta1)*grads[\"dw\"+str(iq)]\n",
        "          m[\"b\"+str(iq)]=beta1*m[\"b\"+str(iq)]+(1-beta1)*grads[\"db\"+str(iq)]\n",
        "          \n",
        "          v[\"w\"+str(iq)]=beta2*v[\"w\"+str(iq)]+(1-beta2)*(grads[\"dw\"+str(iq)])**2\n",
        "          v[\"b\"+str(iq)]=beta2*v[\"b\"+str(iq)]+(1-beta2)*(grads[\"db\"+str(iq)])**2\n",
        "\n",
        "          # Bias Correction:\n",
        "          # calculating mt_hat and vt_hat for weights and biases \n",
        "          mw_hat=m[\"w\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "          mb_hat=m[\"b\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "\n",
        "          vw_hat=v[\"w\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          vb_hat=v[\"b\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          \n",
        "          update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vw_hat+epsilon)),mw_hat)\n",
        "          update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vb_hat+epsilon)),mb_hat)\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "          parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "          parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8S6l8vdT9csw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>NAdam</h3>"
      ],
      "metadata": {
        "id": "wHNnGITB9gvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for nadam***************\n",
        "\n",
        "def nadam(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "  m={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  \n",
        "\n",
        "\n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing m \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    m[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    m[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta1 = 0.9 \n",
        "  beta2=0.999\n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        " \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "          m[\"w\"+str(iq)]=beta1*m[\"w\"+str(iq)]+(1-beta1)*grads[\"dw\"+str(iq)]\n",
        "          m[\"b\"+str(iq)]=beta1*m[\"b\"+str(iq)]+(1-beta1)*grads[\"db\"+str(iq)]\n",
        "          \n",
        "          v[\"w\"+str(iq)]=beta2*v[\"w\"+str(iq)]+(1-beta2)*(grads[\"dw\"+str(iq)])**2\n",
        "          v[\"b\"+str(iq)]=beta2*v[\"b\"+str(iq)]+(1-beta2)*(grads[\"db\"+str(iq)])**2\n",
        "\n",
        "          # Bias Correction:\n",
        "          # calculating mt_hat and vt_hat for weights and biases \n",
        "          mw_hat=m[\"w\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "          mb_hat=m[\"b\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "\n",
        "          vw_hat=v[\"w\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          vb_hat=v[\"b\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          \n",
        "          update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vw_hat+epsilon)),(beta1*mw_hat+(1-beta1)*grads[\"dw\"+str(iq)]))*(1/(1-np.power(beta1,t+1)))\n",
        "          update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vb_hat+epsilon)),(beta1*mb_hat+(1-beta1)*grads[\"db\"+str(iq)]))*(1/(1-np.power(beta1,t+1)))\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "          parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "          parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FMxarSbY9ru4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Predict </h2>\n",
        "Function to predict the labels after training the model"
      ],
      "metadata": {
        "id": "6LQFPsXT9u15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train,y_train,parameters,activation,layer_sizes):\n",
        "\n",
        "  '''This function is used to simple take a model parameters\n",
        "      and run the data points using forward prop, and return the outputs \n",
        "      of all the input data points'''\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    h,a,y_hat = forward_prop(X_train[i], y_train[i], parameters, activation, layer_sizes)\n",
        "\n",
        "    #converting y_hat to a 1d array to match with the y\n",
        "    y_hat = y_hat.flatten()\n",
        "    result.append(y_hat)\n",
        "  \n",
        "  return result\n"
      ],
      "metadata": {
        "id": "OVd1N86DZ06N"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Fit</h2>\n",
        "Function to train the neural network"
      ],
      "metadata": {
        "id": "8Gahdla9-I9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X_train, y_train, layer_sizes, learning_rate = 0.0001, initialization_type = \"random\", activation_function = \"sigmoid\", loss_function = \"cross_entropy\", mini_batch_Size = 32, max_epochs = 5, lambd = 0, optimization_function = mini_batch_gd): \n",
        "\n",
        "\n",
        "\n",
        "  parameters = nn_init(init_type = initialization_type, layer_sizes = layer_sizes)\n",
        "  parameters, train_errors_list, val_errors_list = optimization_function(X_train, y_train,learning_rate, max_epochs, layer_sizes, mini_batch_Size, lambd, loss_function, activation_function, parameters)\n",
        "  print(train_errors_list)\n",
        "  print(val_errors_list)\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aDYOzjtnt48n"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Calling all the functions</h2>"
      ],
      "metadata": {
        "id": "OYdDzZRu-Si9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()"
      ],
      "metadata": {
        "id": "Hsb7BQqXWN-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "  config_defaults = {\n",
        "      'number_hidden_layers': 2,\n",
        "      'number_neurons': 32,\n",
        "      'learning_rate': 0.001,\n",
        "      'initialization_type': \"xavier\",\n",
        "      'activation_function':'sigmoid',\n",
        "      'mini_batch_size' : 64,\n",
        "      'max_epochs': 5,\n",
        "      'lambd': 0,\n",
        "      'optimization_function': \"adam\"\n",
        "      \n",
        "  }\n",
        "\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "\n",
        "  #Forming the layer_sizes i.e., the architecture of our neural network\n",
        "  layer_sizes = [784]\n",
        "  for i in range(config.number_hidden_layers):\n",
        "    layer_sizes = layer_sizes + [config.number_neurons]\n",
        "  layer_sizes  = layer_sizes + [10]\n",
        "\n",
        "  learning_rate = config.learning_rate\n",
        "  initialization_type = config.initialization_type\n",
        "  activation_function = config.activation_function\n",
        "  loss_function = \"cross_entropy\"\n",
        "  mini_batch_size = config.mini_batch_size\n",
        "  max_epochs = config.max_epochs\n",
        "  lambd = config.lambd\n",
        "  opt_fun = config.optimization_function\n",
        "\n",
        "  if opt_fun == \"adam\":\n",
        "    optimization_function = adam\n",
        "  elif opt_fun == \"nadam\":\n",
        "    optimization_function = nadam\n",
        "  elif opt_fun == \"mini_batch_gd\":\n",
        "    optimization_function = mini_batch_gd\n",
        "  elif opt_fun == \"momentum_gd\":\n",
        "    optimization_function = momentum_gd\n",
        "  elif opt_fun == \"nesterov_gd\":\n",
        "    optimization_function = nesterov_gd\n",
        "  elif opt_fun == \"rmsprop\":\n",
        "    optimization_function = rmsprop\n",
        "  else:\n",
        "    print(\"Wrong optimization function\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "\n",
        "  name_run = str(learning_rate) + \"_\" + initialization_type[0] + \"_\" + \\\n",
        "  activation_function[0] + \"_\" + str(mini_batch_size) + \"_\" + str(max_epochs) + \\\n",
        "  \"_\" + str(lambd) + \"_\" + opt_fun[:4]\n",
        "\n",
        "\n",
        "  parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "  wandb.run.name = name_run\n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bLaUCuD_Jsxv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"learning_rate\":{\n",
        "       'values': [0.001, 0.0001]\n",
        "    },\n",
        "\n",
        "    \"number_hidden_layers\": {\n",
        "        'values' : [3, 4, 5]\n",
        "    },\n",
        "\n",
        "    \"number_neurons\": {\n",
        "       'values': [32, 64, 128]\n",
        "    },\n",
        "\n",
        "    \"initialization_type\": {\n",
        "        'values' : [\"xavier\", \"random\"]\n",
        "    },\n",
        "\n",
        "    \"activation_function\": {\n",
        "        'values': [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "    },\n",
        "\n",
        "    \"mini_batch_size\": {\n",
        "        'values': [16,32,64,128]\n",
        "    },\n",
        "\n",
        "    \"max_epochs\": {\n",
        "        'values': [5, 10, 20]\n",
        "    },\n",
        "\n",
        "    \"lambd\": {\n",
        "        'values': [0, 0.0005, 0.5]\n",
        "    },\n",
        "\n",
        "    \"optimization_function\": {\n",
        "        'values': [\"mini_batch_gd\", \"momentum_gd\", \"nesterov_gd\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' :{\n",
        "        'name': 'Validation_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': hyperparameters\n",
        "}"
      ],
      "metadata": {
        "id": "JBZOwcGOip9K"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"safikhan\", project=\"trial 1\")\n",
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "CPV1eT1ZpN1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "84a3733d-1226-4b0a-a623-6883396e1d12"
      },
      "execution_count": 1,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ad3a1e0e88d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msweep_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"safikhan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"trial 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZzVShqd0Jsvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qZZN_1tsqGWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dIreqnQdqGUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZhrLpiWRqGOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1QeeVSeeqGLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EYOmvodmqGI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ADKEbRF9qGDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RtPhg15lqF_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "layer_sizes = [784,32,32,10]\n",
        "learning_rate = 0.0001\n",
        "initialization_type = \"xavier\"\n",
        "activation_function = \"sigmoid\"\n",
        "loss_function = \"cross_entropy\"\n",
        "mini_batch_size = 32\n",
        "max_epochs = 20\n",
        "lambd = 0 \n",
        "optimization_function = adam\n",
        "\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()\n",
        "\n",
        "parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "res = predict(X_train,y_train, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_train, res, lambd, layer_sizes, parameters)\n",
        "acc= calc_accuracy(res,y_train)\n",
        "print(\"Train loss is :\",err)\n",
        "print(\"Train accuracy:\",acc)\n",
        "\n",
        "res = predict(X_test,y_test, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_test, res, lambd, layer_sizes, parameters)\n",
        "acc= calc_accuracy(res,y_test)\n",
        "print(\"Test loss is :\",err)\n",
        "print(\"Test accuracy:\",acc)"
      ],
      "metadata": {
        "id": "pH8uZIeiRodf",
        "outputId": "b2ee9e79-3b11-4919-deed-0769d67b0319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:58<00:00, 26.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.5938793890969736, 1.3811810581496706, 1.2461804001646357, 1.1441376444216969, 1.0628641676235533, 0.9963229939692377, 0.9408294617678099, 0.8938749214678439, 0.8536452779408857, 0.8187876748045251, 0.7882746363735883, 0.7613160877966636, 0.7372984276406996, 0.7157402826267814, 0.6962598273882539, 0.6785508069873908, 0.6623650316748899, 0.6474994789552855, 0.6337866670565659, 0.6210874374102066]\n",
            "Train loss is : 0.6210874374102066\n",
            "Train accuracy: 80.51666666666667\n",
            "Test loss is : 0.6412384003940093\n",
            "Test accuracy: 79.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "layer_sizes = [784,32,32,10]\n",
        "learning_rate = 0.0001\n",
        "initialization_type = \"xavier\"\n",
        "activation_function = \"sigmoid\"\n",
        "loss_function = \"cross_entropy\"\n",
        "mini_batch_size = 32\n",
        "max_epochs = 20\n",
        "lambd = 0 \n",
        "optimization_function = n_adam\n",
        "\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()\n",
        "\n",
        "parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "res = predict(X_train,y_train, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_train, res, lambd, layer_sizes, parameters)\n",
        "print(\"Train loss is :\",err)\n",
        "\n",
        "\n",
        "res = predict(X_test,y_test, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_test, res, lambd, layer_sizes, parameters)\n",
        "print(\"Test loss is :\",err)\n"
      ],
      "metadata": {
        "id": "O53Um2LQfaxM",
        "outputId": "59ad3af5-4d02-4f37-b5dc-289868a7724c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:39<00:00, 19.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6088209769084224, 0.5523804249973134, 0.5285184353646958, 0.5142271645329473, 0.5042819353625889, 0.4967253705491029, 0.4906372046572024, 0.48552448917878577, 0.4810978723198597, 0.4771753414464648, 0.473636056563522, 0.47039626925518757, 0.4673958813740041, 0.46459051288077574, 0.461946598478084, 0.4594382354892736, 0.45704508862681054, 0.4547509564948635, 0.4525427658327407, 0.4504098500635978]\n",
            "Train loss is : 0.4504098500635978\n",
            "Test loss is : 0.4820588075482052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7hNoGpvKgUkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}