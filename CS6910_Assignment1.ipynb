{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " CS6910_Assignment1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safikhanSoofiyani/CS6910-Assignment1/blob/main/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Feed Forward Neural Network</h1>\n",
        "\n",
        "Work Done by:<br>\n",
        "<ul> \n",
        "<li>Mohammed Safi Ur Rahman Khan - CS21M035 </li>\n",
        "<li>Vamsi Sai Krishna Malineni  - OE20S302 </li>"
      ],
      "metadata": {
        "id": "R5q7bg9s5L8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Importing necessary libraries.</h3>"
      ],
      "metadata": {
        "id": "s_fr1Bcc6eOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ahmgbKIg2f0o"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy \n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various Links used till now:\n",
        "<ul>\n",
        "<li> L2 Regularization - https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd\n",
        "</ul>"
      ],
      "metadata": {
        "id": "M3hgrdYAmz7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and importing wandb"
      ],
      "metadata": {
        "id": "OG-qZnHL7Gyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "id": "VVG9Nfzc7DLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6384b0c4-b813-42ad-fe18-a887ecb76bd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 44.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparing dataset</h3>"
      ],
      "metadata": {
        "id": "GU1iINihIbYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data():\n",
        "\n",
        "  '''This function is used to load the data, define the class labels, performing\n",
        "      the train-test-validation split, normalizing the data, flattening each data\n",
        "      point, converting the class labels to one hot encoded vector.\n",
        "\n",
        "      It return all the split data sets '''\n",
        "\n",
        "\n",
        "  # Loading data from online source\n",
        "  (train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()\n",
        "\n",
        "  # Defining labels for data\n",
        "  num_classes = 10\n",
        "  labels=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "  print(\"Number of data points in train data (initially) - \", len(train_x))\n",
        "  print(\"Number of data points in test data (initially) - \", len(test_x))\n",
        "\n",
        "\n",
        "  #performing the train-validation split\n",
        "  train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=40)\n",
        "  \n",
        "\n",
        "  print(\"Shape of each image - 28x28\" )\n",
        "  image_shape=train_x.shape[1]*train_x.shape[2]\n",
        "  print(\"shape of each image (1D) - \",image_shape)\n",
        "  \n",
        "  train_image_count=len(train_x)\n",
        "  val_image_count = len(val_x)\n",
        "  test_image_count=len(test_x)\n",
        "  \n",
        "  # Creating a matrix of image data \n",
        "  # each image is represented as a row by flattening the matrix: converting (60000,28,28) tensor to (60000,784) matrix\n",
        "  X_train=np.zeros((train_image_count,image_shape))\n",
        "  X_val=np.zeros((val_image_count,image_shape))\n",
        "  X_test=np.zeros((test_image_count,image_shape))\n",
        "  \n",
        "  # converting the images into grayscale by normalizing\n",
        "  for i in range(train_image_count):\n",
        "    X_train[i]=(copy.deepcopy(train_x[i].flatten()))/255.0 \n",
        "  for i in range(val_image_count):\n",
        "    X_val[i]=(copy.deepcopy(val_x[i].flatten()))/255.0\n",
        "  for i in range(test_image_count):\n",
        "    X_test[i]=(copy.deepcopy(test_x[i].flatten()))/255.0\n",
        "  \n",
        "\n",
        "\n",
        "  #One hot encoding the label vectors to represent a probability distribution\n",
        "  y_train = np.zeros((train_y.size, 10))\n",
        "  y_train[np.arange(train_y.size), train_y] = 1\n",
        "\n",
        "  y_val = np.zeros((val_y.size, 10))\n",
        "  y_val[np.arange(val_y.size), val_y] = 1\n",
        "\n",
        "  y_test = np.zeros((test_y.size, 10))\n",
        "  y_test[np.arange(test_y.size), test_y] = 1\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  return X_train,X_val,X_test,y_train,y_val,y_test,labels\n",
        "  "
      ],
      "metadata": {
        "id": "uIanRhNjIZiu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting images locally"
      ],
      "metadata": {
        "id": "kM6jDLswWRYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain,xval,xtest,ytrain,yval,ytest,labels=prepare_data()\n",
        "# Creating training dataset\n",
        "train=np.asarray(list(zip(xtrain,ytrain)))\n",
        "# plotting a single image from each class\n",
        "sample_images=[]\n",
        "wandb_arr=[]\n",
        "i=1\n",
        "plt.suptitle(\"Plotting image of each class from Fashion MNIST Dataset\")\n",
        "\n",
        "while(len(sample_images)!=10):\n",
        "  n=random.randrange(0,len(train))\n",
        "  lab_index=np.asarray(np.nonzero(train[n][1]))[0][0]\n",
        "  \n",
        "  if(lab_index not in sample_images):\n",
        "    plt.subplot(3,5,i)\n",
        "    sample_images.append(lab_index)\n",
        "    plt.title(labels[lab_index])\n",
        "    plt.axis(False)\n",
        "    plt.imshow(train[n][0].reshape((28,28)))\n",
        "    i=i+1\n"
      ],
      "metadata": {
        "id": "VmT6fEAtWQzV",
        "outputId": "6c4a5ab6-f0f8-4399-cc37-99b0b60b892c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting images via wandb"
      ],
      "metadata": {
        "id": "nRU6TlqpVSFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#********** Plotting images via wandb************\n",
        "\n",
        "# Enter the entity and project details from wandb.ai\n",
        "wandb.init(entity=\"\",project=\"\")\n",
        "\n",
        "# Loading dataset\n",
        "xtrain,xval,xtest,ytrain,yval,ytest,labels=prepare_data()\n",
        "\n",
        "# Creating training dataset\n",
        "train=np.asarray(list(zip(xtrain,ytrain)))\n",
        "\n",
        "\n",
        "sample_images=[]\n",
        "wandb_arr=[]\n",
        "i=1\n",
        "plt.suptitle(\"Plotting image of each class from Fashion MNIST Dataset\")\n",
        "while(len(sample_images)!=10):\n",
        "  n=random.randrange(0,len(train))\n",
        "  lab_index=np.asarray(np.nonzero(train[n][1]))[0][0]\n",
        "  if(lab_index not in sample_images):\n",
        "    sample_images.append(lab_index)\n",
        "    wandb_arr.append(wandb.Image(train[n][0].reshape((28,28)),caption=labels[train[n][1]]))\n",
        "    i=i+1\n",
        "wandb.log({\"images\":wandb_arr})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "-9BOTbyz3rpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QPmxsns1rJjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()"
      ],
      "metadata": {
        "id": "ntsvInuZrJhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Defining Various Utility functions</h3>"
      ],
      "metadata": {
        "id": "7OI2Z4lEK1Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Initialization Functions</h4>"
      ],
      "metadata": {
        "id": "cBekYpc66E6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Xavier(layer_sizes):\n",
        "  params = {}\n",
        "  for i in range(1,len(layer_sizes)):\n",
        "      norm_xav=np.sqrt(6)/np.sqrt(layer_sizes[i]+layer_sizes[i-1])\n",
        "      params[\"w\"+str(i)]=np.random.randn(layer_sizes[i],layer_sizes[i-1])*norm_xav\n",
        "      params[\"b\"+str(i)]=np.zeros((layer_sizes[i],1))\n",
        "  \n",
        "  return params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nUVnJQfuUHqb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Random(layer_sizes):\n",
        "  params = {}\n",
        "  for i in range(1,len(layer_sizes)):\n",
        "      params[\"w\"+str(i)]=0.01*np.random.randn(layer_sizes[i],layer_sizes[i-1])\n",
        "      params[\"b\"+str(i)]=0.01*np.random.randn(layer_sizes[i],1)\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "1sRkD2gu46dL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Activation Functions </h4>"
      ],
      "metadata": {
        "id": "I7VIhXa56Zi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(pre_act):\n",
        "  try:\n",
        "    return (1.0/(1.0+np.exp(-pre_act)))\n",
        "  except:\n",
        "    print(\"error in sigmoid\")"
      ],
      "metadata": {
        "id": "ZM7quY3l6PZZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(pre_act):\n",
        "  return (np.tanh(pre_act))\n"
      ],
      "metadata": {
        "id": "dAvWjzlz6RjK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(pre_act):\n",
        "  return (np.maximum(0,pre_act))"
      ],
      "metadata": {
        "id": "D8ZpvzQu6TZt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  try:\n",
        "    return(np.exp(x)/np.sum(np.exp(x)))\n",
        "  except:\n",
        "    print(\"error in softmax\")"
      ],
      "metadata": {
        "id": "8Vq4OnO96WRI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hMEinOgW6XqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Derivatives of Activation Functions </h4>"
      ],
      "metadata": {
        "id": "PZcX9QpD6keQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))"
      ],
      "metadata": {
        "id": "6SgvWXU76reg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh_derivative(x):\n",
        "  return 1.0 -tanh(x)**2\n"
      ],
      "metadata": {
        "id": "EFMQdq_b6rcV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(x):\n",
        "  return 1. * (x>0)"
      ],
      "metadata": {
        "id": "bUXYoFkF6rZ-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_derivative(x):\n",
        "  return softmax(x) * (1-softmax(x))"
      ],
      "metadata": {
        "id": "Up46ku0y6rRm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative(A, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    return sigmoid_derivative(A)\n",
        "  elif activation == \"tanh\":\n",
        "    return tanh_derivative(A)\n",
        "  elif activation == \"relu\":\n",
        "    return relu_derivative(A)\n"
      ],
      "metadata": {
        "id": "rHgm-ZbS6OLW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Loss Functions</h4>"
      ],
      "metadata": {
        "id": "56KNthDl67G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y, y_hat):\n",
        "  error = np.sum(((y - y_hat)**2) / (2 * len(y)))\n",
        "  return error"
      ],
      "metadata": {
        "id": "PRnQH-867Ayy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossEntropy(y, y_hat):\n",
        "  error = - np.sum( np.multiply(y , np.log(y_hat)))/len(y)\n",
        "  return error"
      ],
      "metadata": {
        "id": "YP4QzvRF7Xw-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculating loss \n",
        "def loss_calc(loss_name, y, y_hat, lambd, layer_sizes, parameters):\n",
        "  error=0\n",
        "  if(loss_name == \"sse\"):\n",
        "    error=MSE(y, y_hat)\n",
        "  elif(loss_name == \"cross_entropy\"):\n",
        "    error=CrossEntropy(y, y_hat)\n",
        "    #error = -np.sum(np.sum(y_t*np.log(y_hat)))\n",
        "\n",
        "  #For L2 Regularization\n",
        "  regularized_error = 0.0\n",
        "  for i in range(len(layer_sizes)-1, 0, -1):\n",
        "    regularized_error += (np.sum(parameters[\"w\"+str(i)]))**2\n",
        "  regularized_error = error + ((lambd/(2*len(y)))*(regularized_error))\n",
        "\n",
        "\n",
        "  return regularized_error"
      ],
      "metadata": {
        "id": "MyC7PPN16-Sn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4> Accuracy <h4>\n",
        "\n"
      ],
      "metadata": {
        "id": "j1tkJKxj0QzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(res,y_t):\n",
        "    # res is the vector that comes \n",
        "    acc=0.0\n",
        "    \n",
        "    for x in range(len(res)):\n",
        "      if(res[x].argmax()==y_t[x].argmax()):\n",
        "        acc+=1\n",
        "    acc=acc/len(y_t)\n",
        "    return(acc*100)"
      ],
      "metadata": {
        "id": "H-FBfniHt5I_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bghjsd1F6-QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Initialization of Neural Network</h2>"
      ],
      "metadata": {
        "id": "RvRGfWa078JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_init(layer_sizes, init_type = \"random\"):\n",
        "  # Layer Sizes denotes the number of neurons per layer\n",
        "  # 784 is for the input layer. \n",
        "  # 32 is for the hidden layers. \n",
        "  # 10 is for the output layers\n",
        "\n",
        "  # initializing parameters for the neural network, \n",
        "  params={}\n",
        "  if(init_type==\"xavier\"):\n",
        "    params = Xavier(layer_sizes)\n",
        "\n",
        "  elif(init_type==\"random\"):\n",
        "    params = Random(layer_sizes)\n",
        "\n",
        "  else:\n",
        "    print(\"Enter a valid weight initilization type\")\n",
        "\n",
        "  return params\n"
      ],
      "metadata": {
        "id": "2ZICexToBSP0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Forward Propagation</h2>"
      ],
      "metadata": {
        "id": "Noyewh3N8QGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X,y,params,active,layer_sizes):\n",
        "  \n",
        "  # Extracting only the image data not the label for the image data\n",
        "  out=copy.deepcopy(X)\n",
        "  out=out.reshape(-1,1)\n",
        "  \n",
        "  #These are stored just to make it easy to keep track of the indices along with layers.\n",
        "  h=[out] # To save the activations for each neuron in a layer\n",
        "  a=[out] # To save the preactivation for each neuron in a layer\n",
        "\n",
        "  if(active==\"sigmoid\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=sigmoid(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"tanh\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=tanh(out)\n",
        "      h.append(post_a)\n",
        "  \n",
        "  elif(active==\"relu\"):\n",
        "    for i in range(1,len(layer_sizes)-1):\n",
        "      weights=params[\"w\"+str(i)]\n",
        "      biases=params[\"b\"+str(i)]\n",
        "      \n",
        "      out=np.dot(weights,h[i-1])+biases\n",
        "      a.append(out)\n",
        "      post_a=relu(out)\n",
        "      h.append(post_a)       \n",
        "  else:\n",
        "    print(\"Enter a valid activation function\") \n",
        "\n",
        "  # Final step for forward propagation, using softmax.\n",
        "  weights=params[\"w\"+str(len(layer_sizes)-1)]\n",
        "  biases=params[\"b\"+str(len(layer_sizes)-1)]\n",
        "  \n",
        "  out=np.dot(weights,h[len(layer_sizes)-2])+biases\n",
        "  a.append(out)\n",
        "  y_hat=softmax(out)\n",
        "  h.append(y_hat)\n",
        "  \n",
        "  \n",
        "  #in h we  are storing values for layers right from input till output\n",
        "  #h0 is input\n",
        "  #in a we are storing values for layers right from input till output\n",
        "  #a0 is input\n",
        "\n",
        "  return h,a,y_hat"
      ],
      "metadata": {
        "id": "8VaQdyqbrqsO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Back Propagation</h2>"
      ],
      "metadata": {
        "id": "wmjefdiK8cxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(y, y_hat, h, a, params, loss_type, layer_sizes, activation):\n",
        "  \n",
        "  #here both y_hat and y are assumed to be column vectors\n",
        "\n",
        "\n",
        "\n",
        "  grad = {}\n",
        "\n",
        "  if loss_type == \"squared_error\":\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = (y_hat - y)\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = (y_hat - y) * softmax_derivative(a[len(layer_sizes)-1])\n",
        "\n",
        "  elif loss_type == 'cross_entropy':\n",
        "    #Here actually it should be one hot vector. But y does the same job\n",
        "    grad[\"da\"+str(len(layer_sizes)-1)] = -(y-y_hat)\n",
        "    grad[\"dh\"+str(len(layer_sizes)-1)] = -(y/y_hat)\n",
        "\n",
        "  for i in range(len(layer_sizes)-1, 0, -1 ):\n",
        "    #print(i)\n",
        "    #Not considering L2 Regularization here. Instead will cumulate in the update section\n",
        "    grad[\"dw\"+str(i)] = np.dot(grad[\"da\"+str(i)], np.transpose(h[i-1]))\n",
        "    grad[\"db\"+str(i)] = grad[\"da\"+str(i)]\n",
        "\n",
        "    if i > 1:\n",
        "      grad[\"dh\"+str(i-1)] = np.dot(np.transpose(params[\"w\"+str(i)]), grad[\"da\"+str(i)])\n",
        "      grad[\"da\"+str(i-1)] = np.multiply(grad[\"dh\" + str(i-1)], derivative(a[i-1],activation))\n",
        " \n",
        "  return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "kAQEfBMIFfW0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate gradients, batchwise"
      ],
      "metadata": {
        "id": "cYFgtV7ZIOCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function):\n",
        "  grads={}\n",
        "  grads.clear()\n",
        "  #iterate over all the points in the current batch\n",
        "  for j in range(len(X)):\n",
        "    y = np.reshape(Y[j], (-1,1))\n",
        "    #Feed forward the data point\n",
        "    h,a,y_hat = forward_prop(X[j], y, parameters, activation, layers)\n",
        "    #backpropagate the error.\n",
        "    new_grads = back_prop(y,y_hat, h,a, parameters, loss_function, layers, activation)\n",
        "    #keep collecting the gradients for all the data (since vanilla GD)\n",
        "    if j == 0:\n",
        "      grads = copy.deepcopy(new_grads)\n",
        "    else:\n",
        "      for k in range(len(layers)-1,0,-1):\n",
        "        grads[\"dw\"+str(k)] += new_grads[\"dw\"+str(k)]\n",
        "        grads[\"db\"+str(k)] += new_grads[\"db\"+str(k)]\n",
        "  \n",
        "  return grads"
      ],
      "metadata": {
        "id": "CJXUB3SdINv8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Different Optimization Functions</h2>"
      ],
      "metadata": {
        "id": "JhnoYTOQ8g9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Mini Batch Gradient Descent </h3>"
      ],
      "metadata": {
        "id": "KKHa2-Fh8lye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gd(X_train, y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters):\n",
        "  #parameters = nn_init(layers, 'random')\n",
        "  \n",
        "  grads={}\n",
        "  \n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads = grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "    \n",
        "      #Updating the parameters once every one batch\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - (eta * grads[\"dw\"+str(j)])\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - (eta * grads[\"db\"+str(j)])\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "H7a01Svldakv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Momentum Based Gradient Descent </h3>"
      ],
      "metadata": {
        "id": "yPiDf0vS8vsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gd(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "  #parameters = nn_init(layers, 'random')\n",
        "  \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  gamma = 0.9 #Not treating this as a hyperparameter\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "      #Storing the update history for each parameter.\n",
        "      if i == 0 :\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = eta*grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = eta*grads[\"db\"+str(j)]\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (gamma*update_history[\"w\"+str(j)]) + (eta*grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (gamma*update_history[\"b\"+str(j)]) + (eta*grads[\"db\"+str(j)])\n",
        "\n",
        "    \n",
        "      #Updating the parameters once every one batch with the update_history\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "NHgO8wzQ8-qD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Nesterov Accelerated Gradient Descent</h3>"
      ],
      "metadata": {
        "id": "zXL670qB879E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gd(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        " \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  param_lookahead = {}\n",
        "  gamma = 0.9 #not treating this as a hyperparameter.\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      #If it is the first batch, we still dont have the previous history.\n",
        "      #So, lookahead will be same as the current parameters\n",
        "      if i==0:\n",
        "        param_lookahead = copy.deepcopy(parameters)\n",
        "      \n",
        "      #If its not the first batch then we calculate lookahead according to\n",
        "      #the formula.\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          param_lookahead['w'+str(j)] = parameters['w'+str(j)] + (gamma*update_history[\"w\"+str(j)])\n",
        "                                                                  \n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,param_lookahead,activation,layers,loss_function)\n",
        "      \n",
        "      #Storing the update history for each parameter.\n",
        "\n",
        "      #If its the first batch, we dont have any update history yet. So, it will\n",
        "      #be same as the eta*gradients\n",
        "      if i == 0 :\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = eta*grads[\"dw\"+str(j)]\n",
        "          update_history[\"b\"+str(j)] = eta*grads[\"db\"+str(j)]\n",
        "      \n",
        "      #If its not the first batch, we cumulate the update history as per the \n",
        "      #formula.\n",
        "      else:\n",
        "        for j in range(len(layers)-1, 0, -1):\n",
        "          update_history[\"w\"+str(j)] = (gamma*update_history[\"w\"+str(j)]) + (eta*grads[\"dw\"+str(j)])\n",
        "          update_history[\"b\"+str(j)] = (gamma*update_history[\"b\"+str(j)]) + (eta*grads[\"db\"+str(j)])\n",
        "\n",
        "    \n",
        "      #Updating the parameters once every one batch with the update_history\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n"
      ],
      "metadata": {
        "id": "WEglGxnFE3Fg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> RMSprop</h3>"
      ],
      "metadata": {
        "id": "xNB0DGgD9Ck7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for rmsprop***************\n",
        "def rmsprop(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  \n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta = 0.9 \n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "   \n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "        \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "        v[\"w\"+str(iq)]=beta*v[\"w\"+str(iq)]+(1-beta)*grads[\"dw\"+str(iq)]**2\n",
        "        v[\"b\"+str(iq)]=beta*v[\"b\"+str(iq)]+(1-beta)*grads[\"db\"+str(iq)]**2\n",
        "          \n",
        "        update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(v[\"w\"+str(iq)]+epsilon)),grads[\"dw\"+str(iq)])\n",
        "        update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(v[\"b\"+str(iq)]+epsilon)),grads[\"db\"+str(iq)])\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "        parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "        parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22wMGB7d9Cwc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Adam </h3>"
      ],
      "metadata": {
        "id": "31e2lLRk9YSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for adam***************\n",
        "def adam(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "  m={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "\n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing m \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    m[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    m[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta1 = 0.9 \n",
        "  beta2=0.999\n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "      grads.clear()\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        "      \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "          m[\"w\"+str(iq)]=beta1*m[\"w\"+str(iq)]+(1-beta1)*grads[\"dw\"+str(iq)]\n",
        "          m[\"b\"+str(iq)]=beta1*m[\"b\"+str(iq)]+(1-beta1)*grads[\"db\"+str(iq)]\n",
        "          \n",
        "          v[\"w\"+str(iq)]=beta2*v[\"w\"+str(iq)]+(1-beta2)*(grads[\"dw\"+str(iq)])**2\n",
        "          v[\"b\"+str(iq)]=beta2*v[\"b\"+str(iq)]+(1-beta2)*(grads[\"db\"+str(iq)])**2\n",
        "\n",
        "          # Bias Correction:\n",
        "          # calculating mt_hat and vt_hat for weights and biases \n",
        "          mw_hat=m[\"w\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "          mb_hat=m[\"b\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "\n",
        "          vw_hat=v[\"w\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          vb_hat=v[\"b\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          \n",
        "          update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vw_hat+epsilon)),mw_hat)\n",
        "          update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vb_hat+epsilon)),mb_hat)\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "          parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "          parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8S6l8vdT9csw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>NAdam</h3>"
      ],
      "metadata": {
        "id": "wHNnGITB9gvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*********code for nadam***************\n",
        "\n",
        "def nadam(X_train,y_train, eta, max_epochs, layers, mini_batch_size, lambd, loss_function, activation, parameters ):\n",
        "    \n",
        "  grads={}\n",
        "  update_history = {}\n",
        "  v={}\n",
        "  m={}\n",
        "\n",
        "  train_errors_list = []\n",
        "  val_errors_list = []\n",
        "  train_acc_list = []\n",
        "  val_acc_list = []\n",
        "  \n",
        "\n",
        "\n",
        "  # Initializing update_history\n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    update_history[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    update_history[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing m \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    m[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    m[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  # Initializing v \n",
        "  for i in range(len(layers)-1,0,-1):\n",
        "    v[\"w\"+str(i)]=np.zeros((layers[i],layers[i-1]))\n",
        "    v[\"b\"+str(i)]=np.zeros((layers[i],1))\n",
        "  \n",
        "  beta1 = 0.9 \n",
        "  beta2=0.999\n",
        "  epsilon=1e-8\n",
        "\n",
        "  #iterate till max epochs\n",
        "  for t in tqdm(range(max_epochs)):\n",
        "\n",
        "\n",
        "    #iterate over all batches\n",
        "    for i in range(0, len(X_train), mini_batch_size):\n",
        "\n",
        "      grads.clear()\n",
        "\n",
        "      X = X_train[i:i + mini_batch_size]\n",
        "      Y = y_train[i:i + mini_batch_size]\n",
        "      \n",
        "      grads=grad_calculate_batchwise(X,Y,parameters,activation,layers,loss_function)\n",
        " \n",
        "      for iq in range(len(layers)-1,0,-1):\n",
        "          m[\"w\"+str(iq)]=beta1*m[\"w\"+str(iq)]+(1-beta1)*grads[\"dw\"+str(iq)]\n",
        "          m[\"b\"+str(iq)]=beta1*m[\"b\"+str(iq)]+(1-beta1)*grads[\"db\"+str(iq)]\n",
        "          \n",
        "          v[\"w\"+str(iq)]=beta2*v[\"w\"+str(iq)]+(1-beta2)*(grads[\"dw\"+str(iq)])**2\n",
        "          v[\"b\"+str(iq)]=beta2*v[\"b\"+str(iq)]+(1-beta2)*(grads[\"db\"+str(iq)])**2\n",
        "\n",
        "          # Bias Correction:\n",
        "          # calculating mt_hat and vt_hat for weights and biases \n",
        "          mw_hat=m[\"w\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "          mb_hat=m[\"b\"+str(iq)]/(1-np.power(beta1,t+1))\n",
        "\n",
        "          vw_hat=v[\"w\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          vb_hat=v[\"b\"+str(iq)]/(1-np.power(beta2,t+1))\n",
        "          \n",
        "          update_history[\"w\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vw_hat+epsilon)),(beta1*mw_hat+(1-beta1)*grads[\"dw\"+str(iq)]))*(1/(1-np.power(beta1,t+1)))\n",
        "          update_history[\"b\"+str(iq)]=eta*np.multiply(np.reciprocal(np.sqrt(vb_hat+epsilon)),(beta1*mb_hat+(1-beta1)*grads[\"db\"+str(iq)]))*(1/(1-np.power(beta1,t+1)))\n",
        "\n",
        "      for j in range(len(layers)-1,0,-1):\n",
        "          parameters[\"w\"+str(j)] = (1-((eta*lambd)/mini_batch_size))*parameters[\"w\"+str(j)] - update_history[\"w\"+str(j)]\n",
        "          parameters[\"b\"+str(j)] = parameters[\"b\"+str(j)] - update_history[\"b\"+str(j)]\n",
        "\n",
        "    #Calculating train loss and accuracies\n",
        "    res = predict(X_train,y_train,parameters, activation, layers)\n",
        "    train_err = loss_calc(loss_function, y_train, res, lambd, layers, parameters )\n",
        "    train_acc = calc_accuracy(res, y_train)\n",
        "    train_errors_list.append(train_err)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    #Calculating validation loss\n",
        "    res = predict(X_val, y_val, parameters, activation, layers)\n",
        "    val_err = loss_calc(loss_function, y_val, res, lambd, layers, parameters )\n",
        "    val_acc = calc_accuracy(res,y_val)\n",
        "    val_errors_list.append(val_err)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    log_dict = {\"Train_Accuracy\": train_acc, \"Validation_Accuracy\": val_acc, \\\n",
        "                \"Train_Loss\": train_err, \"Validation_loss\": val_err, \"epoch\": t}\n",
        "                \n",
        "    wandb.log(log_dict)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "return parameters, train_errors_list, val_errors_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FMxarSbY9ru4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Predict </h2>\n",
        "Function to predict the labels after training the model"
      ],
      "metadata": {
        "id": "6LQFPsXT9u15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X_train,y_train,parameters,activation,layer_sizes):\n",
        "\n",
        "  '''This function is used to simple take a model parameters\n",
        "      and run the data points using forward prop, and return the outputs \n",
        "      of all the input data points'''\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    h,a,y_hat = forward_prop(X_train[i], y_train[i], parameters, activation, layer_sizes)\n",
        "\n",
        "    #converting y_hat to a 1d array to match with the y\n",
        "    y_hat = y_hat.flatten()\n",
        "    result.append(y_hat)\n",
        "  \n",
        "  return result\n"
      ],
      "metadata": {
        "id": "OVd1N86DZ06N"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Fit</h2>\n",
        "Function to train the neural network"
      ],
      "metadata": {
        "id": "8Gahdla9-I9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X_train, y_train, layer_sizes, learning_rate = 0.0001, initialization_type = \"random\", activation_function = \"sigmoid\", loss_function = \"cross_entropy\", mini_batch_Size = 32, max_epochs = 5, lambd = 0, optimization_function = mini_batch_gd): \n",
        "\n",
        "\n",
        "\n",
        "  parameters = nn_init(init_type = initialization_type, layer_sizes = layer_sizes)\n",
        "  parameters, train_errors_list, val_errors_list = optimization_function(X_train, y_train,learning_rate, max_epochs, layer_sizes, mini_batch_Size, lambd, loss_function, activation_function, parameters)\n",
        "  print(train_errors_list)\n",
        "  print(val_errors_list)\n",
        "\n",
        "  return parameters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aDYOzjtnt48n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Calling all the functions</h2>"
      ],
      "metadata": {
        "id": "OYdDzZRu-Si9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()"
      ],
      "metadata": {
        "id": "Hsb7BQqXWN-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "  config_defaults = {\n",
        "      'number_hidden_layers': 2,\n",
        "      'number_neurons': 32,\n",
        "      'learning_rate': 0.001,\n",
        "      'initialization_type': \"xavier\",\n",
        "      'activation_function':'sigmoid',\n",
        "      'mini_batch_size' : 64,\n",
        "      'max_epochs': 5,\n",
        "      'lambd': 0,\n",
        "      'optimization_function': adam\n",
        "      \n",
        "  }\n",
        "\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "\n",
        "\n",
        "  #Forming the layer_sizes i.e., the architecture of our neural network\n",
        "  layer_sizes = [784]\n",
        "  for i in range(config.number_hidden_layers):\n",
        "    layer_sizes = layer_sizes + [config.number_neurons]\n",
        "  layer_sizes  = layer_sizes + [10]\n",
        "\n",
        "  learning_rate = config.learning_rate\n",
        "  initialization_type = config.initialization_type\n",
        "  activation_function = config.activation_function\n",
        "  loss_function = \"cross_entropy\"\n",
        "  mini_batch_size = config.mini_batch_size\n",
        "  max_epochs = config.max_epochs\n",
        "  lambd = config.lambd\n",
        "  optimization_function = config.optimization_function\n",
        "\n",
        "  name_run = str(learning_rate) + \"_\" + initialization_type[0] + \"_\" + \\\n",
        "  activation_function[0] + \"_\" + str(mini_batch_size) + \"_\" + str(max_epochs) + \\\n",
        "  \"_\" + str(lambd) + \"_\" + optimization_function.__name__[:4]\n",
        "\n",
        "\n",
        "  parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "  wandb.run.name = name_run\n",
        "  wandb.run.save()\n",
        "  wandb.run.finish()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bLaUCuD_Jsxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"learning_rate\":{\n",
        "       'values': [0.001, 0.0001]\n",
        "    },\n",
        "\n",
        "    \"number_hidden_layers\": {\n",
        "        'values' : [3, 4, 5]\n",
        "    },\n",
        "\n",
        "    \"number_neurons\": {\n",
        "       'values': [32, 64, 128]\n",
        "    },\n",
        "\n",
        "    \"initialization_type\": {\n",
        "        'values' : [\"xavier\", \"random\"]\n",
        "    },\n",
        "\n",
        "    \"activation_function\": {\n",
        "        'values': [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "    },\n",
        "\n",
        "    \"mini_batch_size\": {\n",
        "        'values': [16,32,64,128]\n",
        "    },\n",
        "\n",
        "    \"max_epochs\": {\n",
        "        'values': [5, 10, 20]\n",
        "    },\n",
        "\n",
        "    \"lambd\": {\n",
        "        'values': [0, 0.0005, 0.5]\n",
        "    },\n",
        "\n",
        "    \"optimization_function\": {\n",
        "        'values': [mini_batch_gd, momentum_gd, nesterov_gd, rmsprop, adam, nadam]\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' :{\n",
        "        'name': 'Validation_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': hyperparameters\n",
        "}"
      ],
      "metadata": {
        "id": "JBZOwcGOip9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"\", project=\"\")\n",
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "CPV1eT1ZpN1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZzVShqd0Jsvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qZZN_1tsqGWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dIreqnQdqGUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZhrLpiWRqGOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1QeeVSeeqGLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EYOmvodmqGI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ADKEbRF9qGDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RtPhg15lqF_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "layer_sizes = [784,32,32,10]\n",
        "learning_rate = 0.0001\n",
        "initialization_type = \"xavier\"\n",
        "activation_function = \"sigmoid\"\n",
        "loss_function = \"cross_entropy\"\n",
        "mini_batch_size = 32\n",
        "max_epochs = 20\n",
        "lambd = 0 \n",
        "optimization_function = adam\n",
        "\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()\n",
        "\n",
        "parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "res = predict(X_train,y_train, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_train, res, lambd, layer_sizes, parameters)\n",
        "acc= calc_accuracy(res,y_train)\n",
        "print(\"Train loss is :\",err)\n",
        "print(\"Train accuracy:\",acc)\n",
        "\n",
        "res = predict(X_test,y_test, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_test, res, lambd, layer_sizes, parameters)\n",
        "acc= calc_accuracy(res,y_test)\n",
        "print(\"Test loss is :\",err)\n",
        "print(\"Test accuracy:\",acc)"
      ],
      "metadata": {
        "id": "pH8uZIeiRodf",
        "outputId": "b2ee9e79-3b11-4919-deed-0769d67b0319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [08:58<00:00, 26.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.5938793890969736, 1.3811810581496706, 1.2461804001646357, 1.1441376444216969, 1.0628641676235533, 0.9963229939692377, 0.9408294617678099, 0.8938749214678439, 0.8536452779408857, 0.8187876748045251, 0.7882746363735883, 0.7613160877966636, 0.7372984276406996, 0.7157402826267814, 0.6962598273882539, 0.6785508069873908, 0.6623650316748899, 0.6474994789552855, 0.6337866670565659, 0.6210874374102066]\n",
            "Train loss is : 0.6210874374102066\n",
            "Train accuracy: 80.51666666666667\n",
            "Test loss is : 0.6412384003940093\n",
            "Test accuracy: 79.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "layer_sizes = [784,32,32,10]\n",
        "learning_rate = 0.0001\n",
        "initialization_type = \"xavier\"\n",
        "activation_function = \"sigmoid\"\n",
        "loss_function = \"cross_entropy\"\n",
        "mini_batch_size = 32\n",
        "max_epochs = 20\n",
        "lambd = 0 \n",
        "optimization_function = n_adam\n",
        "\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, labels = prepare_data()\n",
        "\n",
        "parameters = fit(X_train, y_train, layer_sizes, learning_rate, initialization_type, activation_function, loss_function, mini_batch_size, max_epochs, lambd, optimization_function)\n",
        "\n",
        "res = predict(X_train,y_train, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_train, res, lambd, layer_sizes, parameters)\n",
        "print(\"Train loss is :\",err)\n",
        "\n",
        "\n",
        "res = predict(X_test,y_test, parameters, activation_function, layer_sizes)\n",
        "err = loss_calc(loss_function, y_test, res, lambd, layer_sizes, parameters)\n",
        "print(\"Test loss is :\",err)\n"
      ],
      "metadata": {
        "id": "O53Um2LQfaxM",
        "outputId": "59ad3af5-4d02-4f37-b5dc-289868a7724c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (initially) -  60000\n",
            "Number of data points in test data (initially) -  10000\n",
            "Shape of each image - 28x28\n",
            "shape of each image (1D) -  784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [06:39<00:00, 19.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6088209769084224, 0.5523804249973134, 0.5285184353646958, 0.5142271645329473, 0.5042819353625889, 0.4967253705491029, 0.4906372046572024, 0.48552448917878577, 0.4810978723198597, 0.4771753414464648, 0.473636056563522, 0.47039626925518757, 0.4673958813740041, 0.46459051288077574, 0.461946598478084, 0.4594382354892736, 0.45704508862681054, 0.4547509564948635, 0.4525427658327407, 0.4504098500635978]\n",
            "Train loss is : 0.4504098500635978\n",
            "Test loss is : 0.4820588075482052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7hNoGpvKgUkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}